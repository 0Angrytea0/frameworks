{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-dxducROV3b"
      },
      "outputs": [],
      "source": [
        "#KGAT_746a8c48819a19cbaf8ca0048244831b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwFJi0V9Oax9",
        "outputId": "10bd2f55-dad7-46e4-e140-0cd9740718fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.33.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Библиотеки"
      ],
      "metadata": {
        "id": "2qoZ6XcSPW0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "kfCyw8xwOb_a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем датасет  \n",
        "Чтение датасета  \n",
        "Приводим таргет к числовому типу, удаляя строки, которые не удалось конвертировать  \n",
        "В House price of unit area есть цена 0, что может помешать модели, поэтому удаляем  \n",
        "Формируем признаки и целевую переменную Таргет - цена Признаки использую все, кроме цены и времени, осталяя только числовые характеристики  \n",
        "Делюсь на обучающую и тестовую выборки  "
      ],
      "metadata": {
        "id": "V_FLMT1rPUfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/kirbysasuke/house-price-prediction-simplified-for-regression\")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "df = pd.read_csv(\"house-price-prediction-simplified-for-regression/Real_Estate.csv\")\n",
        "\n",
        "df[\"House price of unit area\"] = pd.to_numeric(df[\"House price of unit area\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"House price of unit area\"])\n",
        "\n",
        "df = df[df[\"House price of unit area\"] > 0].copy()\n",
        "\n",
        "y = df[\"House price of unit area\"]\n",
        "X = df.drop(columns=[\"House price of unit area\", \"Transaction date\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ekZTn3pOhpH",
        "outputId": "83e98150-40f1-4419-dabc-78aae6044a05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: angrytea\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/kirbysasuke/house-price-prediction-simplified-for-regression\n",
            "Downloading house-price-prediction-simplified-for-regression.zip to ./house-price-prediction-simplified-for-regression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.4k/17.4k [00:00<00:00, 57.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн библиотечного леса"
      ],
      "metadata": {
        "id": "xsGIPFGvPZqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cлучайный лес без тюнинга даёт MAE ≈ 10.44 и RMSE ≈ 11.98, что означает среднюю абсолютную ошибку порядка 10 условных единиц цены за квадратный метр. Коэффициент детерминации R² ≈ 0.21: модель объясняет около 20 % дисперсии целевой переменной, то есть лучше константного предсказания (например, среднего), но запас для улучшения довольно большой"
      ],
      "metadata": {
        "id": "nR8jPeevfcpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "num_cols = X.columns.tolist()\n",
        "\n",
        "preprocessor_rf = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "rf_baseline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_rf),\n",
        "    (\"model\", RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_baseline.fit(X_train, y_train)\n",
        "y_pred_base = rf_baseline.predict(X_test)\n",
        "\n",
        "mae_base = mean_absolute_error(y_test, y_pred_base)\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))\n",
        "r2_base = r2_score(y_test, y_pred_base)\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"MAE:\", mae_base)\n",
        "print(\"RMSE:\", rmse_base)\n",
        "print(\"R^2:\", r2_base)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDXdGzhPcSp",
        "outputId": "f299b470-f4f9-4a7d-ca0c-3ba29e58683c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "MAE: 10.444865883784049\n",
            "RMSE: 11.982470064550997\n",
            "R^2: 0.20616695643691885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 гипотеза - можно улучшить качество за счёт подбора числа деревьев, глубины, минимального размера листа и числа признаков для сплита"
      ],
      "metadata": {
        "id": "M25ijyVfPdzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подбора числа деревьев, глубины, минимального числа объектов в узле и количества признаков при сплите MAE снижается примерно до 9.95, RMSE — до 11.71, а R^2 растёт до ≈ 0.24. Модель начинает чуть лучше приближать реальные цены: ошибка в среднем падает примерно на пол-единицы, а объясняемая доля вариации увеличивается. То есть простая настройка структуры RandomForest (без изменения признаков) уже даёт устойчивое, хоть и не радикальное улучшение качества по сравнению с бейзлайном"
      ],
      "metadata": {
        "id": "tMi795zhfszg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rf_1 = {\n",
        "    \"model__n_estimators\": [100, 300, 500],\n",
        "    \"model__max_depth\": [None, 5, 10, 20],\n",
        "    \"model__min_samples_split\": [2, 5, 10],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4],\n",
        "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
        "}\n",
        "\n",
        "rf_tuned = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_rf),\n",
        "    (\"model\", RandomForestRegressor(\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_gs_1 = GridSearchCV(\n",
        "    estimator=rf_tuned,\n",
        "    param_grid=param_grid_rf_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_gs_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", rf_gs_1.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -rf_gs_1.best_score_)\n",
        "\n",
        "best_rf_1 = rf_gs_1.best_estimator_\n",
        "y_pred_1 = best_rf_1.predict(X_test)\n",
        "\n",
        "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
        "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_1))\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"MAE:\", mae_1)\n",
        "print(\"RMSE:\", rmse_1)\n",
        "print(\"R^2:\", r2_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPzbro1LPkKp",
        "outputId": "12b05496-2903-4071-9d72-c2c1e7190246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 5, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 2, 'model__n_estimators': 300}\n",
            "Лучший MAE на CV: 10.015323727497934\n",
            "\n",
            "Гипотеза 1\n",
            "MAE: 9.951009085203\n",
            "RMSE: 11.71158196402611\n",
            "R^2: 0.241653666067977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 гипотеза - вместо расстояний до станции MRT используем log(1 + distance)"
      ],
      "metadata": {
        "id": "Nm6JehtCPj_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE и RMSE остаются на уровне гипотезы 1, R^2 лишь символически увеличивается. Это говорит о том, что для случайного леса, который сам умеет строить сложные нелинейные разбиения по признакам, логарифмирование одной координаты почти не добавляет новой информации"
      ],
      "metadata": {
        "id": "zbRBiAaaf20w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = df.copy()\n",
        "df_log[\"log_dist_MRT\"] = np.log1p(df_log[\"Distance to the nearest MRT station\"])\n",
        "\n",
        "features_log = [\n",
        "    \"House age\",\n",
        "    \"log_dist_MRT\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X_log = df_log[features_log]\n",
        "y_log = df_log[\"House price of unit area\"]\n",
        "\n",
        "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
        "    X_log, y_log,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocessor_rf_log = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features_log),\n",
        "    ]\n",
        ")\n",
        "\n",
        "rf_log = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_rf_log),\n",
        "    (\"model\", RandomForestRegressor(\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_gs_2 = GridSearchCV(\n",
        "    estimator=rf_log,\n",
        "    param_grid=param_grid_rf_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_gs_2.fit(X_train_log, y_train_log)\n",
        "\n",
        "print(\"Лучшие параметры:\", rf_gs_2.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -rf_gs_2.best_score_)\n",
        "\n",
        "best_rf_2 = rf_gs_2.best_estimator_\n",
        "y_pred_2 = best_rf_2.predict(X_test_log)\n",
        "\n",
        "mae_2 = mean_absolute_error(y_test_log, y_pred_2)\n",
        "rmse_2 = np.sqrt(mean_squared_error(y_test_log, y_pred_2))\n",
        "r2_2 = r2_score(y_test_log, y_pred_2)\n",
        "\n",
        "print(\"\\nRandomForestRegressor + log(расстояния до MRT)\")\n",
        "print(\"MAE:\", mae_2)\n",
        "print(\"RMSE:\", rmse_2)\n",
        "print(\"R^2:\", r2_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEe8fzW_Pyn7",
        "outputId": "23d00b34-c280-4cfb-f41f-9dc15abb30a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 5, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 5, 'model__n_estimators': 300}\n",
            "Лучший MAE на CV: 10.017778655767861\n",
            "\n",
            "RandomForestRegressor + log(расстояния до MRT)\n",
            "MAE: 9.947368960535641\n",
            "RMSE: 11.70524762347358\n",
            "R^2: 0.24247376448058744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 гипотеза - логарифм таргета"
      ],
      "metadata": {
        "id": "lTsTWgJNPycq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При переходе к обучению леса на log(price) и обратном преобразовании предсказаний в исходный масштаб MAE снижается до ≈ 9.52, RMSE — до ≈ 11.19, а R^2 заметно растёт до ≈ 0.31. Это означает, что модель лучше справляется с предсказанием как относительно дешёвых, так и дорогих объектов, так как логарифмирование «сжимает» хвост распределения цен и уменьшает влияние экстремальных значений на обучение. В итоге лог-трансформация целевой переменной даёт наибольший прирост качества среди трёх гипотез и явно улучшает баланс между биасом и дисперсией для RandomForestRegressor"
      ],
      "metadata": {
        "id": "p7b-wQA1f7UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_log_target = np.log1p(df[\"House price of unit area\"])\n",
        "X_target = df.drop(columns=[\"House price of unit area\", \"Transaction date\"])\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_target, y_log_target,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "num_cols_t = X_target.columns.tolist()\n",
        "\n",
        "preprocessor_rf_t = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols_t),\n",
        "    ]\n",
        ")\n",
        "\n",
        "rf_log_y = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_rf_t),\n",
        "    (\"model\", RandomForestRegressor(\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_gs_3 = GridSearchCV(\n",
        "    estimator=rf_log_y,\n",
        "    param_grid=param_grid_rf_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_gs_3.fit(X_train_t, y_train_t)\n",
        "\n",
        "print(\"Лучшие параметры:\", rf_gs_3.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -rf_gs_3.best_score_)\n",
        "\n",
        "best_rf_3 = rf_gs_3.best_estimator_\n",
        "\n",
        "y_pred_log_t = best_rf_3.predict(X_test_t)\n",
        "y_pred_t = np.expm1(y_pred_log_t)\n",
        "\n",
        "y_test_true = np.expm1(y_test_t)\n",
        "\n",
        "mae_3 = mean_absolute_error(y_test_true, y_pred_t)\n",
        "rmse_3 = np.sqrt(mean_squared_error(y_test_true, y_pred_t))\n",
        "r2_3 = r2_score(y_test_true, y_pred_t)\n",
        "\n",
        "print(\"\\nRandomForestRegressor + логарифм таргета\")\n",
        "print(\"MAE:\", mae_3)\n",
        "print(\"RMSE:\", rmse_3)\n",
        "print(\"R^2:\", r2_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO4nt22rP6-H",
        "outputId": "f0cd631c-d9a0-47a3-a3bc-92c59ae1bed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 5, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__min_samples_split': 2, 'model__n_estimators': 500}\n",
            "Лучший MAE на CV: 0.37656717877557905\n",
            "\n",
            "RandomForestRegressor + логарифм таргета\n",
            "MAE: 9.515755249973074\n",
            "RMSE: 11.190621898727409\n",
            "R^2: 0.3076193662805524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственная реализация"
      ],
      "metadata": {
        "id": "8BqeWi9wQFWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MyDecisionTreeRegressor строит дерево жадно: на каждом узле перебирает признаки и пороги, выбирает разбиение, минимизирующее среднеквадратичную ошибку (MSE), рекурсивно делит выборку, пока не достигнут max_depth / min_samples_split / min_samples_leaf, и в листьях хранит среднее значение таргета\n",
        "Для каждого дерева бутстрепно выбирает объекты, случайно подвыбирает подмножество признаков (max_features), обучает своё MyDecisionTreeRegressor и сохраняет набор таких базовых моделей  \n",
        "При предсказании каждое дерево даёт своё прогнозное значение, а финальный ответ случайного леса получается усреднением предсказаний по всем деревьям"
      ],
      "metadata": {
        "id": "4LCA8IFfsKBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDecisionTreeRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        self.n_features_ = n_features\n",
        "        self.tree_ = []\n",
        "\n",
        "        def mse(values):\n",
        "            if values.size == 0:\n",
        "                return 0.0\n",
        "            mean = values.mean()\n",
        "            return np.mean((values - mean) ** 2)\n",
        "\n",
        "        def best_split(X_node, y_node, depth):\n",
        "            n_samples_node, n_features_node = X_node.shape\n",
        "            if n_samples_node < self.min_samples_split:\n",
        "                return None, None, None, None\n",
        "            if self.max_depth is not None and depth >= self.max_depth:\n",
        "                return None, None, None, None\n",
        "\n",
        "            best_feature = None\n",
        "            best_threshold = None\n",
        "            best_score = np.inf\n",
        "            best_left_idx = None\n",
        "            best_right_idx = None\n",
        "\n",
        "            for feature in range(n_features_node):\n",
        "                values = X_node[:, feature]\n",
        "                thresholds = np.unique(values)\n",
        "                for thr in thresholds:\n",
        "                    left_idx = values <= thr\n",
        "                    right_idx = values > thr\n",
        "                    if left_idx.sum() < self.min_samples_leaf or right_idx.sum() < self.min_samples_leaf:\n",
        "                        continue\n",
        "                    mse_left = mse(y_node[left_idx])\n",
        "                    mse_right = mse(y_node[right_idx])\n",
        "                    score = (left_idx.sum() * mse_left + right_idx.sum() * mse_right) / n_samples_node\n",
        "                    if score < best_score:\n",
        "                        best_score = score\n",
        "                        best_feature = feature\n",
        "                        best_threshold = thr\n",
        "                        best_left_idx = left_idx\n",
        "                        best_right_idx = right_idx\n",
        "\n",
        "            if best_feature is None:\n",
        "                return None, None, None, None\n",
        "            return best_feature, best_threshold, best_left_idx, best_right_idx\n",
        "\n",
        "        def build_tree(X_node, y_node, depth):\n",
        "            node = {}\n",
        "            node[\"value\"] = float(y_node.mean())\n",
        "\n",
        "            feature, threshold, left_idx, right_idx = best_split(X_node, y_node, depth)\n",
        "            if feature is None:\n",
        "                node[\"feature\"] = None\n",
        "                node[\"threshold\"] = None\n",
        "                node[\"left\"] = None\n",
        "                node[\"right\"] = None\n",
        "                return node\n",
        "\n",
        "            node[\"feature\"] = feature\n",
        "            node[\"threshold\"] = threshold\n",
        "            node[\"left\"] = build_tree(X_node[left_idx], y_node[left_idx], depth + 1)\n",
        "            node[\"right\"] = build_tree(X_node[right_idx], y_node[right_idx], depth + 1)\n",
        "            return node\n",
        "\n",
        "        self.tree_ = build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        while node[\"feature\"] is not None:\n",
        "            if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "                node = node[\"left\"]\n",
        "            else:\n",
        "                node = node[\"right\"]\n",
        "        return node[\"value\"]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        y_pred = np.zeros(n_samples, dtype=float)\n",
        "        for i in range(n_samples):\n",
        "            y_pred[i] = self._predict_one(X[i], self.tree_)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class MyRandomForestRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features=\"sqrt\",\n",
        "        bootstrap=True,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.bootstrap = bootstrap\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _get_n_features_to_sample(self, n_features):\n",
        "        if isinstance(self.max_features, int):\n",
        "            return min(self.max_features, n_features)\n",
        "        if isinstance(self.max_features, float):\n",
        "            return max(1, int(self.max_features * n_features))\n",
        "        if self.max_features == \"sqrt\":\n",
        "            return max(1, int(np.sqrt(n_features)))\n",
        "        if self.max_features == \"log2\":\n",
        "            return max(1, int(np.log2(n_features)))\n",
        "        return n_features\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        self.trees_ = []\n",
        "        self.features_subsets_ = []\n",
        "\n",
        "        n_features_sub = self._get_n_features_to_sample(n_features)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            if self.bootstrap:\n",
        "                indices = rng.randint(0, n_samples, size=n_samples)\n",
        "            else:\n",
        "                indices = np.arange(n_samples)\n",
        "\n",
        "            feat_indices = rng.choice(n_features, size=n_features_sub, replace=False)\n",
        "\n",
        "            X_boot = X[indices][:, feat_indices]\n",
        "            y_boot = y[indices]\n",
        "\n",
        "            tree = MyDecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                random_state=None\n",
        "            )\n",
        "            tree.fit(X_boot, y_boot)\n",
        "\n",
        "            self.trees_.append(tree)\n",
        "            self.features_subsets_.append(feat_indices)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        preds = np.zeros((n_samples, len(self.trees_)), dtype=float)\n",
        "\n",
        "        for i, (tree, feat_idx) in enumerate(zip(self.trees_, self.features_subsets_)):\n",
        "            preds[:, i] = tree.predict(X[:, feat_idx])\n",
        "\n",
        "        return preds.mean(axis=1)\n"
      ],
      "metadata": {
        "id": "82qac2v3QHRo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн"
      ],
      "metadata": {
        "id": "H98YaYMyQHlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE ≈ 10.09 — в среднем модель ошибается примерно на 10 единиц цены за квадратный метр  \n",
        "RMSE ≈ 12.46 — квадратичная ошибка чуть выше MAE, что говорит о наличии некоторых сильно «выбивающихся» объектов, но без экстремального влияния  \n",
        "R^2 ≈ 0.14 — модель объясняет около 14 % дисперсии таргета, то есть качество пока довольно скромное, но заметно лучше, чем у одиночного решающего дерева, которое сильно переобучалось и давало отрицательный R^2\n"
      ],
      "metadata": {
        "id": "mpmycSzdoPm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = X.columns.tolist()\n",
        "\n",
        "preprocessor_my_rf = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_rf_baseline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_my_rf),\n",
        "    (\"model\", MyRandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features=\"sqrt\",\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "my_rf_baseline.fit(X_train, y_train)\n",
        "y_pred_my_base = my_rf_baseline.predict(X_test)\n",
        "\n",
        "mae_my_base = mean_absolute_error(y_test, y_pred_my_base)\n",
        "rmse_my_base = np.sqrt(mean_squared_error(y_test, y_pred_my_base))\n",
        "r2_my_base = r2_score(y_test, y_pred_my_base)\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"MAE:\", mae_my_base)\n",
        "print(\"RMSE:\", rmse_my_base)\n",
        "print(\"R^2:\", r2_my_base)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSFED8y0QI-C",
        "outputId": "dca97206-9892-4948-f172-2a87739cdfec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "MAE: 10.090213531174243\n",
            "RMSE: 12.455470340974834\n",
            "R^2: 0.1422578915277779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 гипотеза"
      ],
      "metadata": {
        "id": "3L8VcaYUQJLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучшие параметры дают совсем небольшой прирост по R^2 (с ≈0.14 до ≈0.15) и чуть более низкий MAE, то есть тюнинг помогает, но эффект очень умеренный из-за ограниченности данных"
      ],
      "metadata": {
        "id": "o2IyqIoTz7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = X.columns.tolist()\n",
        "\n",
        "preprocessor_my_rf = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "param_grid_my_rf_1 = {\n",
        "    \"model__n_estimators\": [100, 300, 500],\n",
        "    \"model__max_depth\": [None, 5, 10, 20],\n",
        "    \"model__min_samples_split\": [2, 5, 10],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4],\n",
        "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
        "}\n",
        "\n",
        "my_rf_tuned_1 = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_my_rf),\n",
        "    (\"model\", MyRandomForestRegressor(\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "my_rf_gs_1 = GridSearchCV(\n",
        "    estimator=my_rf_tuned_1,\n",
        "    param_grid=param_grid_my_rf_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_rf_gs_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_rf_gs_1.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_rf_gs_1.best_score_)\n",
        "\n",
        "best_my_rf_1 = my_rf_gs_1.best_estimator_\n",
        "y_pred_my_1 = best_my_rf_1.predict(X_test)\n",
        "\n",
        "mae_my_1 = mean_absolute_error(y_test, y_pred_my_1)\n",
        "rmse_my_1 = np.sqrt(mean_squared_error(y_test, y_pred_my_1))\n",
        "r2_my_1 = r2_score(y_test, y_pred_my_1)\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"MAE:\", mae_my_1)\n",
        "print(\"RMSE:\", rmse_my_1)\n",
        "print(\"R^2:\", r2_my_1)\n"
      ],
      "metadata": {
        "id": "c-lvCj5nQKTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3398c540-02a2-401a-aca7-6579b8756115"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 10, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 10.323365456032814\n",
            "\n",
            "Гипотеза 1\n",
            "MAE: 10.061651862392814\n",
            "RMSE: 12.366973051530215\n",
            "R^2: 0.15440326755218836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 гипотеза"
      ],
      "metadata": {
        "id": "QaqYyz1NQKfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Качество чуть лучше бейзлайна и сопоставимо с гипотезой 1, но логарифмирование расстояния само по себе даёт только небольшой прирост по R^2 и MAE"
      ],
      "metadata": {
        "id": "cUdPFo5e_EJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = df.copy()\n",
        "df_log[\"log_dist_MRT\"] = np.log1p(df_log[\"Distance to the nearest MRT station\"])\n",
        "\n",
        "features_log = [\n",
        "    \"House age\",\n",
        "    \"log_dist_MRT\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X_log = df_log[features_log]\n",
        "y_log = df_log[\"House price of unit area\"]\n",
        "\n",
        "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
        "    X_log, y_log,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocessor_my_rf_log = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features_log),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_rf_tuned_2 = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_my_rf_log),\n",
        "    (\"model\", MyRandomForestRegressor(\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "my_rf_gs_2 = GridSearchCV(\n",
        "    estimator=my_rf_tuned_2,\n",
        "    param_grid=param_grid_my_rf_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_rf_gs_2.fit(X_train_log, y_train_log)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_rf_gs_2.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_rf_gs_2.best_score_)\n",
        "\n",
        "best_my_rf_2 = my_rf_gs_2.best_estimator_\n",
        "y_pred_my_2 = best_my_rf_2.predict(X_test_log)\n",
        "\n",
        "mae_my_2 = mean_absolute_error(y_test_log, y_pred_my_2)\n",
        "rmse_my_2 = np.sqrt(mean_squared_error(y_test_log, y_pred_my_2))\n",
        "r2_my_2 = r2_score(y_test_log, y_pred_my_2)\n",
        "\n",
        "print(\"\\nMyRandomForestRegressor + log(расстояния до MRT)\")\n",
        "print(\"MAE:\", mae_my_2)\n",
        "print(\"RMSE:\", rmse_my_2)\n",
        "print(\"R^2:\", r2_my_2)\n"
      ],
      "metadata": {
        "id": "EPNN7eXyQLfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb888cb-e7ba-426f-90ce-295f70032c6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 10, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 10.323365456032814\n",
            "\n",
            "MyRandomForestRegressor + log(расстояния до MRT)\n",
            "MAE: 10.061651862392814\n",
            "RMSE: 12.366973051530215\n",
            "R^2: 0.15440326755218836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 гипотеза"
      ],
      "metadata": {
        "id": "qYGifZGWQMB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В результате MAE и RMSE заметно уменьшаются, а R^2 растёт до ~0.256 — это лучшая конфигурация для моего леса по сравнению с бейзлайном и предыдущими гипотезами"
      ],
      "metadata": {
        "id": "De-VF1M-_toh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_log_target = np.log1p(df[\"House price of unit area\"])\n",
        "X_target = df.drop(columns=[\"House price of unit area\", \"Transaction date\"])\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_target, y_log_target,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "num_cols_t = X_target.columns.tolist()\n",
        "\n",
        "preprocessor_my_rf_t = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols_t),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_rf_tuned_3 = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_my_rf_t),\n",
        "    (\"model\", MyRandomForestRegressor(\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "my_rf_gs_3 = GridSearchCV(\n",
        "    estimator=my_rf_tuned_3,\n",
        "    param_grid=param_grid_my_rf_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_rf_gs_3.fit(X_train_t, y_train_t)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_rf_gs_3.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_rf_gs_3.best_score_)\n",
        "\n",
        "best_my_rf_3 = my_rf_gs_3.best_estimator_\n",
        "\n",
        "y_pred_log_t = best_my_rf_3.predict(X_test_t)\n",
        "y_pred_t = np.expm1(y_pred_log_t)\n",
        "\n",
        "y_test_true = np.expm1(y_test_t)\n",
        "\n",
        "mae_my_3 = mean_absolute_error(y_test_true, y_pred_t)\n",
        "rmse_my_3 = np.sqrt(mean_squared_error(y_test_true, y_pred_t))\n",
        "r2_my_3 = r2_score(y_test_true, y_pred_t)\n",
        "\n",
        "print(\"\\nMyRandomForestRegressor + логарифм таргета\")\n",
        "print(\"MAE:\", mae_my_3)\n",
        "print(\"RMSE:\", rmse_my_3)\n",
        "print(\"R^2:\", r2_my_3)"
      ],
      "metadata": {
        "id": "jZxOaMhqQM_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d371059-79cb-4dc7-9a47-659a5b8d7858"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 10, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 0.3856605445046651\n",
            "\n",
            "MyRandomForestRegressor + логарифм таргета\n",
            "MAE: 9.687211893877322\n",
            "RMSE: 11.60197299478937\n",
            "R^2: 0.2557820036281576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В задаче регрессии по ценам жилья случайный лес ожидаемо даёт устойчивое качество: библиотечный RandomForestRegressor после подбора гиперпараметров и логарифмирования таргета выходит примерно на MAE ≈ 9.5 и R^2 ≈ 0.31, моя реализация леса показывает похожие тенденции, но остаётся немного слабее (MAE ≈ 9.7, R^2 ≈ 0.26). Во всех вариантах простой тюнинг гиперпараметров даёт умеренный прирост качества, логарифмирование расстояния до MRT почти не меняет картину, а логарифмирование таргета даёт самый заметный выигрыш по метрикам. В целом можно сказать, что и библиотечный, и собственный лес хорошо улавливают нелинейные зависимости в данных, а моя реализация ведёт себя адекватно, но ожидаемо чуть проигрывает оптимизированной реализации sklearn по точности"
      ],
      "metadata": {
        "id": "r1nmw6CV_14x"
      }
    }
  ]
}