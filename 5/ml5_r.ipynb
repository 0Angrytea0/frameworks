{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#KGAT_746a8c48819a19cbaf8ca0048244831b"
      ],
      "metadata": {
        "id": "l-WhW4GRXRWo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEP-I0TyXMvz",
        "outputId": "c1958be5-36f8-4cee-f6db-4a1f9f3d06d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.33.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Библиотеки"
      ],
      "metadata": {
        "id": "CGO7z0n5XmuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "Tw_xhOTwXUQ8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем датасет  \n",
        "Чтение датасета  \n",
        "Приводим таргет к числовому типу, удаляя строки, которые не удалось конвертировать  \n",
        "В House price of unit area есть цена 0, что может помешать модели, поэтому удаляем  "
      ],
      "metadata": {
        "id": "Klkmtn-sXk4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/kirbysasuke/house-price-prediction-simplified-for-regression\")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "df = pd.read_csv(\"house-price-prediction-simplified-for-regression/Real_Estate.csv\")\n",
        "\n",
        "df[\"House price of unit area\"] = pd.to_numeric(df[\"House price of unit area\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"House price of unit area\"])\n",
        "\n",
        "df = df[df[\"House price of unit area\"] > 0].copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hh9XRN-XjQq",
        "outputId": "52da0250-2052-4b98-94ea-d17b9da5ac9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: angrytea\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/kirbysasuke/house-price-prediction-simplified-for-regression\n",
            "Downloading house-price-prediction-simplified-for-regression.zip to ./house-price-prediction-simplified-for-regression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.4k/17.4k [00:00<00:00, 60.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формирую матрицу признаков для регрессии по ценам недвижимости и строю бейзлайновую модель GradientBoostingRegressor  \n",
        "В качестве признаков беру возраст дома, расстояние до MRT, число магазинов, широту и долготу  \n",
        "Числовые признаки стандартизую, далее обучаю бустинг с параметрами по умолчанию"
      ],
      "metadata": {
        "id": "ktuPohVhZBio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Базовый градиентный бустинг без подбора гиперпараметров даёт MAE ≈ 10.55, RMSE ≈ 12.38 и R² ≈ 0.15. То есть модель уже что-то учит, но объясняет вариацию цен довольно слабо и ошибается в среднем на ~10 единиц цены за квадратный метр"
      ],
      "metadata": {
        "id": "FSHogbZxwuDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "    \"House age\",\n",
        "    \"Distance to the nearest MRT station\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X = df[features]\n",
        "y = df[\"House price of unit area\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocess_gb = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gb_base = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_gb),\n",
        "    (\"model\", GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "gb_base.fit(X_train, y_train)\n",
        "y_pred_base = gb_base.predict(X_test)\n",
        "\n",
        "mae_base = mean_absolute_error(y_test, y_pred_base)\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))\n",
        "r2_base = r2_score(y_test, y_pred_base)\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"MAE:\", mae_base)\n",
        "print(\"RMSE:\", rmse_base)\n",
        "print(\"R^2:\", r2_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc6j9o2aY45t",
        "outputId": "eb1cc80e-cd35-4584-8a83-51809cab6896"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "MAE: 10.552408093514218\n",
            "RMSE: 12.376984270914132\n",
            "R^2: 0.1530336711587641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 1 - подбор основных гиперпараметров: числа деревьев, скорости обучения и глубины базовых деревьев"
      ],
      "metadata": {
        "id": "kilITn-sYE5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подбора гиперпараметров (более мелкие деревья и меньший learning_rate) MAE снижается до ≈ 10.09, RMSE до ≈ 11.76, R^2 растёт до ≈ 0.236. Ошибка чуть уменьшается, модель становится более сглаженной и стабильной, но прорыва по метрикам нет - это аккуратное, но не радикальное улучшение по сравнению с бейзлайном"
      ],
      "metadata": {
        "id": "aeqxXywZwxKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_gb_1 = {\n",
        "    \"model__n_estimators\": [100, 200, 300],\n",
        "    \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
        "    \"model__max_depth\": [2, 3, 4]\n",
        "}\n",
        "\n",
        "gb_gs_1 = GridSearchCV(\n",
        "    estimator=gb_base,\n",
        "    param_grid=param_grid_gb_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_gs_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", gb_gs_1.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -gb_gs_1.best_score_)\n",
        "\n",
        "best_gb_1 = gb_gs_1.best_estimator_\n",
        "y_pred_1 = best_gb_1.predict(X_test)\n",
        "\n",
        "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
        "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_1))\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"MAE:\", mae_1)\n",
        "print(\"RMSE:\", rmse_1)\n",
        "print(\"R^2:\", r2_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyjucfmYZIbh",
        "outputId": "d21c612e-3ab6-4611-81e7-b9ab551d3181"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 9.993212187679118\n",
            "\n",
            "Гипотеза 1\n",
            "MAE: 10.090392425795702\n",
            "RMSE: 11.75693830650001\n",
            "R^2: 0.23576847996758077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 2 - расстояние до MRT имеет сильно скошенное распределение, поэтому вместо «сырых» расстояний попробую использовать логарифм расстояния"
      ],
      "metadata": {
        "id": "Ch2OmYVKZQuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логарифмирование расстояния до метро при тех же гиперпараметрах даёт те же самые значения метрик, что и гипотеза 1. Это означает, что для градиентного бустинга на этом датасете такая трансформация признака «Distance to MRT» по сути не добавляет информации: деревья и так умеют хорошо работать с исходными значениями и не выигрывают от log-преобразования"
      ],
      "metadata": {
        "id": "VBcdxiznw3En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = df.copy()\n",
        "df_log[\"log_dist_MRT\"] = np.log1p(df_log[\"Distance to the nearest MRT station\"])\n",
        "\n",
        "features_log = [\n",
        "    \"House age\",\n",
        "    \"log_dist_MRT\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X_log = df_log[features_log]\n",
        "y_log = df_log[\"House price of unit area\"]\n",
        "\n",
        "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
        "    X_log, y_log,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocess_gb_log = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features_log),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gb_log = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_gb_log),\n",
        "    (\"model\", GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_gb_2 = param_grid_gb_1\n",
        "\n",
        "gb_gs_2 = GridSearchCV(\n",
        "    estimator=gb_log,\n",
        "    param_grid=param_grid_gb_2,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_gs_2.fit(X_train_log, y_train_log)\n",
        "\n",
        "print(\"Лучшие параметры:\", gb_gs_2.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -gb_gs_2.best_score_)\n",
        "\n",
        "best_gb_2 = gb_gs_2.best_estimator_\n",
        "y_pred_2 = best_gb_2.predict(X_test_log)\n",
        "\n",
        "mae_2 = mean_absolute_error(y_test_log, y_pred_2)\n",
        "rmse_2 = np.sqrt(mean_squared_error(y_test_log, y_pred_2))\n",
        "r2_2 = r2_score(y_test_log, y_pred_2)\n",
        "\n",
        "print(\"\\nGradientBoostingRegressor + log(расстояния до MRT)\")\n",
        "print(\"MAE:\", mae_2)\n",
        "print(\"RMSE:\", rmse_2)\n",
        "print(\"R^2:\", r2_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snKqtV6sZVuc",
        "outputId": "03b0e6e2-3f44-457c-e3d8-2565f5eb2518"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 9.993212187679118\n",
            "\n",
            "GradientBoostingRegressor + log(расстояния до MRT)\n",
            "MAE: 10.090392425795702\n",
            "RMSE: 11.75693830650001\n",
            "R^2: 0.23576847996758077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 гипотеза - логарифмирование таргета"
      ],
      "metadata": {
        "id": "wkeFIb_CZiFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Когда мы переходим к прогнозированию логарифма цены (а потом возвращаемся к исходной шкале), MAE падает до ≈ 9.87, RMSE — до ≈ 11.52, а R^2 растёт до ≈ 0.27. Логарифмирование таргета помогает сгладить влияние дорогих объектов и сделать распределение ошибок более равномерным, поэтому именно эта конфигурация даёт лучшую точность среди всех проверенных вариантов sklearn-модели"
      ],
      "metadata": {
        "id": "x0BgYQn9w7bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_log_target = np.log1p(df[\"House price of unit area\"])\n",
        "X_base = df[features]\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_base, y_log_target,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocess_gb_t = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gb_log_y = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_gb_t),\n",
        "    (\"model\", GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_gb_3 = param_grid_gb_1\n",
        "\n",
        "gb_gs_3 = GridSearchCV(\n",
        "    estimator=gb_log_y,\n",
        "    param_grid=param_grid_gb_3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_gs_3.fit(X_train_t, y_train_t)\n",
        "\n",
        "print(\"Лучшие параметры:\", gb_gs_3.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -gb_gs_3.best_score_)\n",
        "\n",
        "best_gb_3 = gb_gs_3.best_estimator_\n",
        "\n",
        "y_pred_log = best_gb_3.predict(X_test_t)\n",
        "y_pred_3 = np.expm1(y_pred_log)\n",
        "\n",
        "y_test_true = np.expm1(y_test_t)\n",
        "\n",
        "mae_3 = mean_absolute_error(y_test_true, y_pred_3)\n",
        "rmse_3 = np.sqrt(mean_squared_error(y_test_true, y_pred_3))\n",
        "r2_3 = r2_score(y_test_true, y_pred_3)\n",
        "\n",
        "print(\"\\nGradientBoostingRegressor + логарифм таргета\")\n",
        "print(\"MAE:\", mae_3)\n",
        "print(\"RMSE:\", rmse_3)\n",
        "print(\"R^2:\", r2_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6uKiVPNZkb9",
        "outputId": "070fa8f5-78fe-475b-8bdc-b193fad3d439"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 0.363232610566802\n",
            "\n",
            "GradientBoostingRegressor + логарифм таргета\n",
            "MAE: 9.868580230321113\n",
            "RMSE: 11.519322207002325\n",
            "R^2: 0.26634763910905235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализую свои версии деревьев и градиентного бустинга для регрессии  \n",
        "Сначала пишу MyDecisionTreeRegressor: это простое дерево решений, которое на каждом шаге перебирает признаки и пороги, ищет разбиение, минимизирующее среднеквадратичную ошибку (MSE), и в листьях хранит среднее значение таргета.\n",
        "Затем делаю MyRandomForestRegressor как ансамбль из таких деревьев: для каждого дерева беру бутстрап-выборку объектов и случайное подмножество признаков, обучаю дерево и потом усредняю предсказания всех деревьев  \n",
        "Наконец, реализую MyGradientBoostingRegressor  \n",
        "1) начинаю с константного предсказания (среднее по y)\n",
        "2) на каждой итерации считаю остатки y - current_pred и обучаю новое дерево на этих остатках\n",
        "3) добавляю его вклад к текущему предсказанию с шагом learning_rate.\n",
        "В итоге получаю свой градиентный бустинг по MSE, который по интерфейсу ведёт себя как обычный регрессионный алгоритм из sklearn и дальше используется в пайплайне с тем же препроцессингом, что и библиотечная модель"
      ],
      "metadata": {
        "id": "XjK6RmsXZoTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDecisionTreeRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        self.n_features_ = n_features\n",
        "        self.tree_ = []\n",
        "\n",
        "        def mse(values):\n",
        "            if values.size == 0:\n",
        "                return 0.0\n",
        "            mean = values.mean()\n",
        "            return np.mean((values - mean) ** 2)\n",
        "\n",
        "        def best_split(X_node, y_node, depth):\n",
        "            n_samples_node, n_features_node = X_node.shape\n",
        "            if n_samples_node < self.min_samples_split:\n",
        "                return None, None, None, None\n",
        "            if self.max_depth is not None and depth >= self.max_depth:\n",
        "                return None, None, None, None\n",
        "\n",
        "            best_feature = None\n",
        "            best_threshold = None\n",
        "            best_score = np.inf\n",
        "            best_left_idx = None\n",
        "            best_right_idx = None\n",
        "\n",
        "            for feature in range(n_features_node):\n",
        "                values = X_node[:, feature]\n",
        "                thresholds = np.unique(values)\n",
        "                for thr in thresholds:\n",
        "                    left_idx = values <= thr\n",
        "                    right_idx = values > thr\n",
        "                    if left_idx.sum() < self.min_samples_leaf or right_idx.sum() < self.min_samples_leaf:\n",
        "                        continue\n",
        "                    mse_left = mse(y_node[left_idx])\n",
        "                    mse_right = mse(y_node[right_idx])\n",
        "                    score = (left_idx.sum() * mse_left + right_idx.sum() * mse_right) / n_samples_node\n",
        "                    if score < best_score:\n",
        "                        best_score = score\n",
        "                        best_feature = feature\n",
        "                        best_threshold = thr\n",
        "                        best_left_idx = left_idx\n",
        "                        best_right_idx = right_idx\n",
        "\n",
        "            if best_feature is None:\n",
        "                return None, None, None, None\n",
        "            return best_feature, best_threshold, best_left_idx, best_right_idx\n",
        "\n",
        "        def build_tree(X_node, y_node, depth):\n",
        "            node = {}\n",
        "            node[\"value\"] = float(y_node.mean())\n",
        "\n",
        "            feature, threshold, left_idx, right_idx = best_split(X_node, y_node, depth)\n",
        "            if feature is None:\n",
        "                node[\"feature\"] = None\n",
        "                node[\"threshold\"] = None\n",
        "                node[\"left\"] = None\n",
        "                node[\"right\"] = None\n",
        "                return node\n",
        "\n",
        "            node[\"feature\"] = feature\n",
        "            node[\"threshold\"] = threshold\n",
        "            node[\"left\"] = build_tree(X_node[left_idx], y_node[left_idx], depth + 1)\n",
        "            node[\"right\"] = build_tree(X_node[right_idx], y_node[right_idx], depth + 1)\n",
        "            return node\n",
        "\n",
        "        self.tree_ = build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        while node[\"feature\"] is not None:\n",
        "            if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "                node = node[\"left\"]\n",
        "            else:\n",
        "                node = node[\"right\"]\n",
        "        return node[\"value\"]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        y_pred = np.zeros(n_samples, dtype=float)\n",
        "        for i in range(n_samples):\n",
        "            y_pred[i] = self._predict_one(X[i], self.tree_)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class MyRandomForestRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features=\"sqrt\",\n",
        "        bootstrap=True,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.bootstrap = bootstrap\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _get_n_features_to_sample(self, n_features):\n",
        "        if isinstance(self.max_features, int):\n",
        "            return min(self.max_features, n_features)\n",
        "        if isinstance(self.max_features, float):\n",
        "            return max(1, int(self.max_features * n_features))\n",
        "        if self.max_features == \"sqrt\":\n",
        "            return max(1, int(np.sqrt(n_features)))\n",
        "        if self.max_features == \"log2\":\n",
        "            return max(1, int(np.log2(n_features)))\n",
        "        return n_features\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        self.trees_ = []\n",
        "        self.features_subsets_ = []\n",
        "\n",
        "        n_features_sub = self._get_n_features_to_sample(n_features)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            if self.bootstrap:\n",
        "                indices = rng.randint(0, n_samples, size=n_samples)\n",
        "            else:\n",
        "                indices = np.arange(n_samples)\n",
        "\n",
        "            feat_indices = rng.choice(n_features, size=n_features_sub, replace=False)\n",
        "\n",
        "            X_boot = X[indices][:, feat_indices]\n",
        "            y_boot = y[indices]\n",
        "\n",
        "            tree = MyDecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                random_state=None\n",
        "            )\n",
        "            tree.fit(X_boot, y_boot)\n",
        "\n",
        "            self.trees_.append(tree)\n",
        "            self.features_subsets_.append(feat_indices)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        preds = np.zeros((n_samples, len(self.trees_)), dtype=float)\n",
        "\n",
        "        for i, (tree, feat_idx) in enumerate(zip(self.trees_, self.features_subsets_)):\n",
        "            preds[:, i] = tree.predict(X[:, feat_idx])\n",
        "\n",
        "        return preds.mean(axis=1)\n",
        "\n",
        "class MyGradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y, dtype=float)\n",
        "\n",
        "        self.init_ = np.mean(y)\n",
        "        self.trees_ = []\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        current_pred = np.full(n_samples, self.init_, dtype=float)\n",
        "\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "\n",
        "        for m in range(self.n_estimators):\n",
        "            residuals = y - current_pred\n",
        "\n",
        "            tree = MyDecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                random_state=None if self.random_state is None else self.random_state + m\n",
        "            )\n",
        "\n",
        "            tree.fit(X, residuals)\n",
        "            self.trees_.append(tree)\n",
        "\n",
        "            update = tree.predict(X)\n",
        "            current_pred += self.learning_rate * update\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        y_pred = np.full(X.shape[0], self.init_, dtype=float)\n",
        "\n",
        "        for tree in self.trees_:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "x4JM8d-eaJuI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн"
      ],
      "metadata": {
        "id": "c_OK9Gi5aQvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В базовой конфигурации своя реализация даёт MAE ≈ 10.62, RMSE ≈ 12.40 и R^2 ≈ 0.15. Это очень близко к библиотечному бейзлайну (чуть хуже по ошибкам), что уже показывает, что общий алгоритм реализован адекватно"
      ],
      "metadata": {
        "id": "lzfMjzHkxBHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "    \"House age\",\n",
        "    \"Distance to the nearest MRT station\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X = df[features]\n",
        "y = df[\"House price of unit area\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocess_my_gb = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_gb_base = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_my_gb),\n",
        "    (\"model\", MyGradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "my_gb_base.fit(X_train, y_train)\n",
        "y_pred_my_base = my_gb_base.predict(X_test)\n",
        "\n",
        "mae_my_base = mean_absolute_error(y_test, y_pred_my_base)\n",
        "rmse_my_base = np.sqrt(mean_squared_error(y_test, y_pred_my_base))\n",
        "r2_my_base = r2_score(y_test, y_pred_my_base)\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"MAE:\", mae_my_base)\n",
        "print(\"RMSE:\", rmse_my_base)\n",
        "print(\"R^2:\", r2_my_base)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtBu7VtwaSMz",
        "outputId": "e1ba0103-c47b-470e-a5ff-be1197f791c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "MAE: 10.618062606022761\n",
            "RMSE: 12.395913639515339\n",
            "R^2: 0.15044098822927754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 1"
      ],
      "metadata": {
        "id": "W3Nuz6zTaSmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подбора n_estimators / max_depth / learning_rate MAE снижается до ≈ 10.08, RMSE — до ≈ 11.75, R^2 поднимается до ≈ 0.237. Поведение почти полностью совпадает со sklearn-версией: аккуратный выигрыш по качеству за счёт более удачных настроек, без кардинального скачка"
      ],
      "metadata": {
        "id": "Ww4DK4kmxFpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_my_gb_1 = {\n",
        "    \"model__n_estimators\": [100, 200, 300],\n",
        "    \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
        "    \"model__max_depth\": [2, 3, 4]\n",
        "}\n",
        "\n",
        "my_gb_gs_1 = GridSearchCV(\n",
        "    estimator=my_gb_base,\n",
        "    param_grid=param_grid_my_gb_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_gb_gs_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_gb_gs_1.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_gb_gs_1.best_score_)\n",
        "\n",
        "best_my_gb_1 = my_gb_gs_1.best_estimator_\n",
        "y_pred_my_1 = best_my_gb_1.predict(X_test)\n",
        "\n",
        "mae_my_1 = mean_absolute_error(y_test, y_pred_my_1)\n",
        "rmse_my_1 = np.sqrt(mean_squared_error(y_test, y_pred_my_1))\n",
        "r2_my_1 = r2_score(y_test, y_pred_my_1)\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"MAE:\", mae_my_1)\n",
        "print(\"RMSE:\", rmse_my_1)\n",
        "print(\"R^2:\", r2_my_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9zThabSaTnP",
        "outputId": "d48010e5-0b60-4555-b1b7-a2db727d9d71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 10.065510997281969\n",
            "\n",
            "Гипотеза 1\n",
            "MAE: 10.080777640697098\n",
            "RMSE: 11.746663273588833\n",
            "R^2: 0.23710370389214264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 2"
      ],
      "metadata": {
        "id": "UAMN7lyfaT0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логарифмирование расстояния до MRT в собственной реализации снова даёт идентичные метрики гипотезе 1. Значит, как и в случае со sklearn-моделью, эта трансформация для бустинга на данном датасете не играет заметной роли, модель уже умеет «переваривать» исходный масштаб признака"
      ],
      "metadata": {
        "id": "i86yzEYoxJrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = df.copy()\n",
        "df_log[\"log_dist_MRT\"] = np.log1p(df_log[\"Distance to the nearest MRT station\"])\n",
        "\n",
        "features_log = [\n",
        "    \"House age\",\n",
        "    \"log_dist_MRT\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X_log = df_log[features_log]\n",
        "y_log = df_log[\"House price of unit area\"]\n",
        "\n",
        "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
        "    X_log, y_log,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocess_my_gb_log = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features_log),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_gb_log = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_my_gb_log),\n",
        "    (\"model\", MyGradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_my_gb_2 = param_grid_my_gb_1\n",
        "\n",
        "my_gb_gs_2 = GridSearchCV(\n",
        "    estimator=my_gb_log,\n",
        "    param_grid=param_grid_my_gb_2,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_gb_gs_2.fit(X_train_log, y_train_log)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_gb_gs_2.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_gb_gs_2.best_score_)\n",
        "\n",
        "best_my_gb_2 = my_gb_gs_2.best_estimator_\n",
        "y_pred_my_2 = best_my_gb_2.predict(X_test_log)\n",
        "\n",
        "mae_my_2 = mean_absolute_error(y_test_log, y_pred_my_2)\n",
        "rmse_my_2 = np.sqrt(mean_squared_error(y_test_log, y_pred_my_2))\n",
        "r2_my_2 = r2_score(y_test_log, y_pred_my_2)\n",
        "\n",
        "print(\"\\nMyGradientBoostingRegressor + log(расстояния до MRT)\")\n",
        "print(\"MAE:\", mae_my_2)\n",
        "print(\"RMSE:\", rmse_my_2)\n",
        "print(\"R^2:\", r2_my_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siJ8zravaUuP",
        "outputId": "ef7c3e0e-cbae-4d0d-9cac-7722d8a3b1ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 10.065510997281969\n",
            "\n",
            "MyGradientBoostingRegressor + log(расстояния до MRT)\n",
            "MAE: 10.080777640697098\n",
            "RMSE: 11.746663273588833\n",
            "R^2: 0.23710370389214264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 3"
      ],
      "metadata": {
        "id": "_oNcLaGoaU_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При работе с логарифмом таргета MAE снижается до ≈ 9.75, RMSE — до ≈ 11.46, а R^2 растёт до ≈ 0.274. Это лучший вариант среди всех конфигураций собственной реализации и по динамике метрик он очень близок к библиотечному бустингу. То есть логарифмирование целевой переменной снова оказывается наиболее полезным приёмом, а совпадение поведения с sklearn говорит, что MyGradientBoostingRegressor реализован корректно и воспроизводит ту же логику обучения, что и стандартная библиотечная модель"
      ],
      "metadata": {
        "id": "j0JjuYsUxRQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_log_target = np.log1p(df[\"House price of unit area\"])\n",
        "X_base = df[features]\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_base, y_log_target,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "preprocess_my_gb_t = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_gb_log_y = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_my_gb_t),\n",
        "    (\"model\", MyGradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_my_gb_3 = param_grid_my_gb_1\n",
        "\n",
        "my_gb_gs_3 = GridSearchCV(\n",
        "    estimator=my_gb_log_y,\n",
        "    param_grid=param_grid_my_gb_3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_gb_gs_3.fit(X_train_t, y_train_t)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_gb_gs_3.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_gb_gs_3.best_score_)\n",
        "\n",
        "best_my_gb_3 = my_gb_gs_3.best_estimator_\n",
        "\n",
        "y_pred_log_my = best_my_gb_3.predict(X_test_t)\n",
        "y_pred_my_3 = np.expm1(y_pred_log_my)\n",
        "\n",
        "y_test_true = np.expm1(y_test_t)\n",
        "\n",
        "mae_my_3 = mean_absolute_error(y_test_true, y_pred_my_3)\n",
        "rmse_my_3 = np.sqrt(mean_squared_error(y_test_true, y_pred_my_3))\n",
        "r2_my_3 = r2_score(y_test_true, y_pred_my_3)\n",
        "\n",
        "print(\"\\nMyGradientBoostingRegressor + логарифм таргета\")\n",
        "print(\"MAE:\", mae_my_3)\n",
        "print(\"RMSE:\", rmse_my_3)\n",
        "print(\"R^2:\", r2_my_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0IZ9xnaWx4",
        "outputId": "6a592d29-d0e2-45a0-8436-60d32cb2f46a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 100}\n",
            "Лучший MAE на CV: 0.3691941098657465\n",
            "\n",
            "MyGradientBoostingRegressor + логарифм таргета\n",
            "MAE: 9.746978165936188\n",
            "RMSE: 11.460055094202803\n",
            "R^2: 0.2738775267227396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайны у sklearn и моей реализации дают MAE ≈ 10–10.6 и R² около 0.15, после подбора гиперпараметров качество немного улучшается, но не радикально. Логарифм расстояния до MRT почти ничего не даёт, а вот логарифмирование таргета даёт заметный выигрыш: MAE падает до ~9.7–9.9, R^2 растёт до ~0.27. При этом моя реализация ведёт себя очень похоже на библиотечный GradientBoostingRegressor, так что её можно считать корректной — ключевую роль в улучшении играет не алгоритм как таковой, а грамотная работа с таргетом"
      ],
      "metadata": {
        "id": "exmXXXFjaWb9"
      }
    }
  ]
}