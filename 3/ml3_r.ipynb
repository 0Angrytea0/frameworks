{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rTAgWtrSA-l5"
      },
      "outputs": [],
      "source": [
        "#KGAT_746a8c48819a19cbaf8ca0048244831b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te4DR5oUFQyZ",
        "outputId": "71932c79-cb77-4320-8f67-87d474379eb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Библиотеки"
      ],
      "metadata": {
        "id": "5JzEmGFOBUtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
      ],
      "metadata": {
        "id": "ISkbKDTlBUY8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем датасет  \n",
        "Чтение датасета  \n",
        "Приводим таргет к числовому типу, удаляя строки, которые не удалось конвертировать  \n",
        "В House price of unit area есть цена 0, что может помешать модели, поэтому удаляем  \n",
        "Формируем признаки и целевую переменную Таргет - цена Признаки использую все, кроме цены и времени, осталяя только числовые характеристики  \n",
        "Делюсь на обучающую и тестовую выборки"
      ],
      "metadata": {
        "id": "ODF8AFEFBWsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/kirbysasuke/house-price-prediction-simplified-for-regression\")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "df = pd.read_csv(\"house-price-prediction-simplified-for-regression/Real_Estate.csv\")\n",
        "\n",
        "df[\"House price of unit area\"] = pd.to_numeric(df[\"House price of unit area\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"House price of unit area\"])\n",
        "\n",
        "df = df[df[\"House price of unit area\"] > 0].copy()\n",
        "\n",
        "y = df[\"House price of unit area\"]\n",
        "X = df.drop(columns=[\"House price of unit area\", \"Transaction date\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krE9qRN5BE95",
        "outputId": "2a57ba52-62ab-4883-a114-15492bf1c67e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: angrytea\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/kirbysasuke/house-price-prediction-simplified-for-regression\n",
            "Downloading house-price-prediction-simplified-for-regression.zip to ./house-price-prediction-simplified-for-regression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.4k/17.4k [00:00<00:00, 16.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строю базовую модель дерева решений для регрессии с настройками по умолчанию"
      ],
      "metadata": {
        "id": "0LBfJdk8EXS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Базовое дерево решений без подбора гиперпараметров даёт очень плохое качество: MAE ≈ 15.8, RMSE ≈ 18.6 и сильно отрицательный R^2 (≈ -0.91). Это значит, что модель на тесте работает даже хуже, чем тривиальный предиктор «всегда средняя цена». Такое дерево явно переобучается на небольшой выборке и практически бесполезно как регрессионная модель"
      ],
      "metadata": {
        "id": "lXb6LUe4H5_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_base = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "tree_base.fit(X_train, y_train)\n",
        "y_pred_base = tree_base.predict(X_test)\n",
        "\n",
        "mae_base = mean_absolute_error(y_test, y_pred_base)\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))\n",
        "r2_base = r2_score(y_test, y_pred_base)\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"MAE:\", mae_base)\n",
        "print(\"RMSE:\", rmse_base)\n",
        "print(\"R^2:\", r2_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O654YIT9EW5I",
        "outputId": "291bab67-52de-499b-898a-d4a61a5f9c47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "MAE: 15.793130604968212\n",
            "RMSE: 18.57892447102105\n",
            "R^2: -0.9084360232822504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 1 - аккуратная регуляризация дерева (ограничение глубины и минимального числа объектов в узлах) поможет снизить переобучение и улучшить MAE на тесте. С помощью GridSearchCV подбираю max_depth, min_samples_split и min_samples_leaf, оптимизируя MAE"
      ],
      "metadata": {
        "id": "sh7cMjC9Eh6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подбора глубины и минимального числа объектов в узле/листе качество заметно улучшается относительно бейзлайна: MAE падает с ~15.8 до ~12.1, RMSE тоже заметно уменьшается. R^2 вырастает с -0.91 до примерно -0.15 - всё ещё хуже константной модели, но уже не провал. Гипотеза частично подтверждается: жёсткий контроль структуры дерева действительно снижает переобучение и делает предсказания разумнее, но одного только тюнинга max_depth/min_samples_* всё равно недостаточно, чтобы получить действительно хорошую регрессию на этих данных"
      ],
      "metadata": {
        "id": "1wB4BAjRIBkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_tree = {\n",
        "    \"model__max_depth\": [None, 3, 5, 7, 9, 12],\n",
        "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4, 8],\n",
        "}\n",
        "\n",
        "tree_pipe = Pipeline(steps=[\n",
        "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "tree_gs = GridSearchCV(\n",
        "    estimator=tree_pipe,\n",
        "    param_grid=param_grid_tree,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "tree_gs.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", tree_gs.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -tree_gs.best_score_)\n",
        "\n",
        "best_tree_1 = tree_gs.best_estimator_\n",
        "y_pred_1 = best_tree_1.predict(X_test)\n",
        "\n",
        "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
        "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_1))\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"MAE:\", mae_1)\n",
        "print(\"RMSE:\", rmse_1)\n",
        "print(\"R^2:\", r2_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TdvAM64EdnV",
        "outputId": "2a35147d-a6b6-4295-81a0-f81dabc03706"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 7, 'model__min_samples_leaf': 4, 'model__min_samples_split': 20}\n",
            "Лучший MAE на CV: 10.26159260635103\n",
            "\n",
            "Гипотеза 1\n",
            "MAE: 12.096726754385621\n",
            "RMSE: 14.397518280072877\n",
            "R^2: -0.14607158350214489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 2 - добавляю новый столбец log_dist_MRT, использую его вместо исходного расстояния и снова провожу подбор гиперпараметров дерева"
      ],
      "metadata": {
        "id": "BDXnYth4EvwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучшие параметры и итоговые метрики полностью совпали с гипотезой 1: MAE ≈ 12.1, RMSE ≈ 14.4, R^2 ≈ -0.15. Это говорит о том, что в текущей конфигурации дерево либо почти не использует признак расстояния в качестве разбиения, либо его линейная/логарифмическая форма принципиально не влияет на структуру дерева. Гипотеза про log(distance) по сути не приносит дополнительного выигрыша по сравнению с уже настроенной моделью"
      ],
      "metadata": {
        "id": "whtA2tyFIE0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = df.copy()\n",
        "df_log[\"log_dist_MRT\"] = np.log1p(df_log[\"Distance to the nearest MRT station\"])\n",
        "\n",
        "features_log = [\n",
        "    \"House age\",\n",
        "    \"log_dist_MRT\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X_log = df_log[features_log]\n",
        "y_log = df_log[\"House price of unit area\"]\n",
        "\n",
        "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
        "    X_log, y_log,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tree_log_pipe = Pipeline(steps=[\n",
        "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_tree_log = param_grid_tree\n",
        "\n",
        "tree_log_gs = GridSearchCV(\n",
        "    estimator=tree_log_pipe,\n",
        "    param_grid=param_grid_tree_log,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "tree_log_gs.fit(X_train_log, y_train_log)\n",
        "\n",
        "print(\"Лучшие параметры:\", tree_log_gs.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -tree_log_gs.best_score_)\n",
        "\n",
        "best_tree_2 = tree_log_gs.best_estimator_\n",
        "y_pred_2 = best_tree_2.predict(X_test_log)\n",
        "\n",
        "mae_2 = mean_absolute_error(y_test_log, y_pred_2)\n",
        "rmse_2 = np.sqrt(mean_squared_error(y_test_log, y_pred_2))\n",
        "r2_2 = r2_score(y_test_log, y_pred_2)\n",
        "\n",
        "print(\"\\nDecisionTreeRegressor + log(расстояния до MRT)\")\n",
        "print(\"MAE:\", mae_2)\n",
        "print(\"RMSE:\", rmse_2)\n",
        "print(\"R^2:\", r2_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBj873c0E2wr",
        "outputId": "f53fd35d-6402-42ea-e6df-825f9682d605"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 7, 'model__min_samples_leaf': 4, 'model__min_samples_split': 20}\n",
            "Лучший MAE на CV: 10.26159260635103\n",
            "\n",
            "DecisionTreeRegressor + log(расстояния до MRT)\n",
            "MAE: 12.096726754385621\n",
            "RMSE: 14.397518280072877\n",
            "R^2: -0.14607158350214489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 3 - цены могут иметь тяжёлый хвост (очень дорогие квартиры), поэтому логарифмирование таргета снижает влияние экстремально дорогих объектов"
      ],
      "metadata": {
        "id": "V_dne_MyE87o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логарифмирование целевой переменной даёт уже качественно другой результат: MAE снижается примерно до 9.68, RMSE — до ~11.49, а R^2 становится положительным (около 0.27). Это лучший результат среди всех вариантов дерева решений в этой лабораторной, и по уровню качества он уже сравним с линейными моделями и KNN. Дерево в лог-шкале лучше справляется с разбросом цен и меньше «ломается» на дорогих объектах. Гипотеза подтверждается: переход к логарифму цены делает задачу более удобной для дерева решений и заметно улучшает его обобщающую способность"
      ],
      "metadata": {
        "id": "M6n4gRgEIhWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_log_target = np.log1p(df[\"House price of unit area\"])\n",
        "X_t = df.drop(columns=[\"House price of unit area\", \"Transaction date\"])\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_t, y_log_target,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tree_logy_pipe = Pipeline(steps=[\n",
        "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_tree_logy = param_grid_tree\n",
        "\n",
        "tree_logy_gs = GridSearchCV(\n",
        "    estimator=tree_logy_pipe,\n",
        "    param_grid=param_grid_tree_logy,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "tree_logy_gs.fit(X_train_t, y_train_t)\n",
        "\n",
        "print(\"Лучшие параметры:\", tree_logy_gs.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -tree_logy_gs.best_score_)\n",
        "\n",
        "best_tree_3 = tree_logy_gs.best_estimator_\n",
        "\n",
        "y_pred_log_t = best_tree_3.predict(X_test_t)\n",
        "y_pred_t = np.expm1(y_pred_log_t)\n",
        "\n",
        "y_test_true = np.expm1(y_test_t)\n",
        "\n",
        "mae_3 = mean_absolute_error(y_test_true, y_pred_t)\n",
        "rmse_3 = np.sqrt(mean_squared_error(y_test_true, y_pred_t))\n",
        "r2_3 = r2_score(y_test_true, y_pred_t)\n",
        "\n",
        "print(\"\\nDecisionTreeRegressor + логарифм таргета\")\n",
        "print(\"MAE:\", mae_3)\n",
        "print(\"RMSE:\", rmse_3)\n",
        "print(\"R^2:\", r2_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3fnAh7rFDc2",
        "outputId": "2526ebe6-e475-45c1-a8e1-14ead009ed32"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 3, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2}\n",
            "Лучший MAE на CV: 0.38411858154593076\n",
            "\n",
            "DecisionTreeRegressor + логарифм таргета\n",
            "MAE: 9.683404490734594\n",
            "RMSE: 11.487476850632104\n",
            "R^2: 0.2703984202546508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственная реализация  \n",
        "Модель устроена по схеме CART: в методе fit рекурсивно строю дерево, на каждом узле перебираю признаки и пороги разбиения, считаю уменьшение среднеквадратичной ошибки (MSE) и выбираю лучший сплит. Рост дерева останавливается по условиям max_depth, min_samples_split, min_samples_leaf или если дальнейшее разбиение не даёт выигрыша по метрике. В листьях хранится среднее значение таргета по объектам, попавшим в лист. В методе predict для каждого объекта последовательно спускаюсь от корня по условиям feature <= threshold до листа и возвращаю сохранённое среднее. Параметры работают аналогично DecisionTreeRegressor из sklearn, поэтому модель можно использовать внутри Pipeline и настраивать через GridSearchCV"
      ],
      "metadata": {
        "id": "5JqMUhK0Fdhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDecisionTreeRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self,\n",
        "                 max_depth=None,\n",
        "                 min_samples_split=2,\n",
        "                 min_samples_leaf=1,\n",
        "                 random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "\n",
        "    class Node:\n",
        "        def __init__(self, is_leaf, prediction=None,\n",
        "                     feature_index=None, threshold=None,\n",
        "                     left=None, right=None):\n",
        "            self.is_leaf = is_leaf\n",
        "            self.prediction = prediction\n",
        "            self.feature_index = feature_index\n",
        "            self.threshold = threshold\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "\n",
        "    def _mse(self, y):\n",
        "        if y.size == 0:\n",
        "            return 0.0\n",
        "        mean = np.mean(y)\n",
        "        return np.mean((y - mean) ** 2)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return None, None\n",
        "\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "        best_mse = self._mse(y)\n",
        "        base_mse = best_mse\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            x_col = X[:, feature]\n",
        "            sort_idx = np.argsort(x_col)\n",
        "            x_sorted = x_col[sort_idx]\n",
        "            y_sorted = y[sort_idx]\n",
        "\n",
        "            unique_vals = np.unique(x_sorted)\n",
        "            if unique_vals.size <= 1:\n",
        "                continue\n",
        "\n",
        "            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n",
        "\n",
        "            for thr in thresholds:\n",
        "                left_mask = x_sorted <= thr\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                y_left = y_sorted[left_mask]\n",
        "                y_right = y_sorted[right_mask]\n",
        "\n",
        "                if y_left.size < self.min_samples_leaf or y_right.size < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                mse_left = self._mse(y_left)\n",
        "                mse_right = self._mse(y_right)\n",
        "                mse_split = (y_left.size * mse_left + y_right.size * mse_right) / n_samples\n",
        "\n",
        "                if mse_split < best_mse:\n",
        "                    best_mse = mse_split\n",
        "                    best_feature = feature\n",
        "                    best_threshold = thr\n",
        "\n",
        "        if best_feature is None:\n",
        "            return None, None\n",
        "\n",
        "        if best_mse >= base_mse:\n",
        "            return None, None\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        n_samples = y.size\n",
        "        prediction = np.mean(y)\n",
        "\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return self.Node(is_leaf=True, prediction=prediction)\n",
        "\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return self.Node(is_leaf=True, prediction=prediction)\n",
        "\n",
        "        feature, threshold = self._best_split(X, y)\n",
        "        if feature is None:\n",
        "            return self.Node(is_leaf=True, prediction=prediction)\n",
        "\n",
        "        left_mask = X[:, feature] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        X_left, y_left = X[left_mask], y[left_mask]\n",
        "        X_right, y_right = X[right_mask], y[right_mask]\n",
        "\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return self.Node(\n",
        "            is_leaf=False,\n",
        "            prediction=None,\n",
        "            feature_index=feature,\n",
        "            threshold=threshold,\n",
        "            left=left_child,\n",
        "            right=right_child\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        self.root_ = self._build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _predict_row(self, x, node):\n",
        "        if node.is_leaf:\n",
        "            return node.prediction\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict_row(x, node.left)\n",
        "        else:\n",
        "            return self._predict_row(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        preds = np.array([self._predict_row(row, self.root_) for row in X])\n",
        "        return preds"
      ],
      "metadata": {
        "id": "aRSEc2isFdVX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн собственной реализации"
      ],
      "metadata": {
        "id": "mP5OjNjcF1cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственное дерево решений в базовой конфигурации даёт почти те же метрики, что и библиотечное: MAE ≈ 15.8, RMSE ≈ 18.6, R^2 ≈ -0.90. То есть на тесте модель всё ещё сильно хуже константного предиктора по средней цене. Это значит, что без ограничения глубины и размеров узлов дерево жёстко переобучается на небольшой выборке"
      ],
      "metadata": {
        "id": "3WnmmwgHIxE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tree_base = MyDecisionTreeRegressor(\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1\n",
        ")\n",
        "\n",
        "my_tree_base.fit(X_train.values, y_train.values)\n",
        "y_pred_my_base = my_tree_base.predict(X_test.values)\n",
        "\n",
        "mae_my_base = mean_absolute_error(y_test, y_pred_my_base)\n",
        "rmse_my_base = np.sqrt(mean_squared_error(y_test, y_pred_my_base))\n",
        "r2_my_base = r2_score(y_test, y_pred_my_base)\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"MAE:\", mae_my_base)\n",
        "print(\"RMSE:\", rmse_my_base)\n",
        "print(\"R^2:\", r2_my_base)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHgqtR6vF1Ba",
        "outputId": "c517b261-3a53-4c90-a789-cea55b6fd0d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "MAE: 15.776151832857598\n",
            "RMSE: 18.55395950475414\n",
            "R^2: -0.9033106425561108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 1"
      ],
      "metadata": {
        "id": "5UG_rb3IF4uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подбора max_depth, min_samples_split и min_samples_leaf качество становится существенно лучше: MAE падает примерно до 12.1, RMSE до ~14.4, а R² поднимается с ~ -0.90 до ~ -0.15. Как и в случае со sklearn-деревом, модель всё ещё формально хуже константной по R^2, но уже не катастрофически. Гипотеза подтверждается: аккуратный контроль структуры дерева заметно снижает переобучение и делает предсказания более адекватными, хотя до действительно хорошего качества всё ещё далеко"
      ],
      "metadata": {
        "id": "joeHv5H9I57s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tree_pipe = Pipeline(steps=[\n",
        "    (\"model\", MyDecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "param_grid_my_tree_1 = {\n",
        "    \"model__max_depth\": [None, 3, 5, 7, 9, 12],\n",
        "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4, 8],\n",
        "}\n",
        "\n",
        "my_tree_gs_1 = GridSearchCV(\n",
        "    estimator=my_tree_pipe,\n",
        "    param_grid=param_grid_my_tree_1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_tree_gs_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучший параметры:\", my_tree_gs_1.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_tree_gs_1.best_score_)\n",
        "\n",
        "best_my_tree_1 = my_tree_gs_1.best_estimator_\n",
        "y_pred_my_1 = best_my_tree_1.predict(X_test)\n",
        "\n",
        "mae_my_1 = mean_absolute_error(y_test, y_pred_my_1)\n",
        "rmse_my_1 = np.sqrt(mean_squared_error(y_test, y_pred_my_1))\n",
        "r2_my_1 = r2_score(y_test, y_pred_my_1)\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"MAE:\", mae_my_1)\n",
        "print(\"RMSE:\", rmse_my_1)\n",
        "print(\"R^2:\", r2_my_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nqTsliIF6x4",
        "outputId": "224371b6-ccc3-441c-c614-95214f23061a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучший параметры: {'model__max_depth': 7, 'model__min_samples_leaf': 4, 'model__min_samples_split': 20}\n",
            "Лучший MAE на CV: 10.26159260635103\n",
            "\n",
            "Гипотеза 1\n",
            "MAE: 12.096726754385621\n",
            "RMSE: 14.397518280072875\n",
            "R^2: -0.14607158350214466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 2"
      ],
      "metadata": {
        "id": "p1BnDjzDF7Ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавление логарифма расстояния до MRT к собственной реализации приводит к идентичным лучшим гиперпараметрам и тем же метрикам, что и в гипотезе 1: MAE ≈ 12.1, RMSE ≈ 14.4, R^2 ≈ -0.15. По сути дерево либо почти не использует признак расстояния как ключевой для разбиений, либо форма расстояния (линейная vs log1p) мало что меняет в структуре. Гипотеза про log(distance) для дерева решений в этой задаче эффекта не даёт — качество осталось на уровне «только тюнинг структуры»"
      ],
      "metadata": {
        "id": "HLSBfmwOJGLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log2 = df.copy()\n",
        "df_log2[\"log_dist_MRT\"] = np.log1p(df_log2[\"Distance to the nearest MRT station\"])\n",
        "\n",
        "features_log2 = [\n",
        "    \"House age\",\n",
        "    \"log_dist_MRT\",\n",
        "    \"Number of convenience stores\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "]\n",
        "\n",
        "X_log2 = df_log2[features_log2]\n",
        "y_log2 = df_log2[\"House price of unit area\"]\n",
        "\n",
        "X_train_log2, X_test_log2, y_train_log2, y_test_log2 = train_test_split(\n",
        "    X_log2, y_log2,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "my_tree_pipe_2 = Pipeline(steps=[\n",
        "    (\"model\", MyDecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "param_grid_my_tree_2 = param_grid_my_tree_1\n",
        "\n",
        "my_tree_gs_2 = GridSearchCV(\n",
        "    estimator=my_tree_pipe_2,\n",
        "    param_grid=param_grid_my_tree_2,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_tree_gs_2.fit(X_train_log2, y_train_log2)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_tree_gs_2.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_tree_gs_2.best_score_)\n",
        "\n",
        "best_my_tree_2 = my_tree_gs_2.best_estimator_\n",
        "y_pred_my_2 = best_my_tree_2.predict(X_test_log2)\n",
        "\n",
        "mae_my_2 = mean_absolute_error(y_test_log2, y_pred_my_2)\n",
        "rmse_my_2 = np.sqrt(mean_squared_error(y_test_log2, y_pred_my_2))\n",
        "r2_my_2 = r2_score(y_test_log2, y_pred_my_2)\n",
        "\n",
        "print(\"\\nMyDecisionTreeRegressor + log(расстояния до MRT)\")\n",
        "print(\"MAE:\", mae_my_2)\n",
        "print(\"RMSE:\", rmse_my_2)\n",
        "print(\"R^2:\", r2_my_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJKRgGgnF8fS",
        "outputId": "d30361b7-a599-4324-bc77-4d540f610575"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 7, 'model__min_samples_leaf': 4, 'model__min_samples_split': 20}\n",
            "Лучший MAE на CV: 10.26159260635103\n",
            "\n",
            "MyDecisionTreeRegressor + log(расстояния до MRT)\n",
            "MAE: 12.096726754385621\n",
            "RMSE: 14.397518280072875\n",
            "R^2: -0.14607158350214466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гипотеза 3"
      ],
      "metadata": {
        "id": "4iNGIRrpF94o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При переходе к логарифму целевой переменной качество резко вырастает: MAE ≈ 9.65, RMSE ≈ 11.48, R^2 ≈ 0.27 - фактически те же значения, что и у библиотечного DecisionTreeRegressor с той же идеей. Модель наконец-то становится лучше константного предиктора и уже приближается по качеству к результатам линейных моделей и KNN. Гипотеза полностью подтверждается: логарифмирование цены сглаживает влияние дорогих объектов и делает задачу более удобной для кусочно-постоянных предсказаний дерева"
      ],
      "metadata": {
        "id": "XSzaqlogJaqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_logtarget2 = np.log1p(df[\"House price of unit area\"])\n",
        "X_t2 = df.drop(columns=[\"House price of unit area\", \"Transaction date\"])\n",
        "\n",
        "X_train_t2, X_test_t2, y_train_t2, y_test_t2 = train_test_split(\n",
        "    X_t2, y_logtarget2,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "my_tree_pipe_3 = Pipeline(steps=[\n",
        "    (\"model\", MyDecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "param_grid_my_tree_3 = param_grid_my_tree_1\n",
        "\n",
        "my_tree_gs_3 = GridSearchCV(\n",
        "    estimator=my_tree_pipe_3,\n",
        "    param_grid=param_grid_my_tree_3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "my_tree_gs_3.fit(X_train_t2, y_train_t2)\n",
        "\n",
        "print(\"Лучшие параметры:\", my_tree_gs_3.best_params_)\n",
        "print(\"Лучший MAE на CV:\", -my_tree_gs_3.best_score_)\n",
        "\n",
        "best_my_tree_3 = my_tree_gs_3.best_estimator_\n",
        "\n",
        "y_pred_log_t2 = best_my_tree_3.predict(X_test_t2)\n",
        "y_pred_t2 = np.expm1(y_pred_log_t2)\n",
        "\n",
        "y_test_true2 = np.expm1(y_test_t2)\n",
        "\n",
        "mae_my_3 = mean_absolute_error(y_test_true2, y_pred_t2)\n",
        "rmse_my_3 = np.sqrt(mean_squared_error(y_test_true2, y_pred_t2))\n",
        "r2_my_3 = r2_score(y_test_true2, y_pred_t2)\n",
        "\n",
        "print(\"\\nMyDecisionTreeRegressor + логарифм таргета\")\n",
        "print(\"MAE:\", mae_my_3)\n",
        "print(\"RMSE:\", rmse_my_3)\n",
        "print(\"R^2:\", r2_my_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klf9G0gzF-zB",
        "outputId": "5ee50464-ee08-4df9-a76c-e17f6fde71f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'model__max_depth': 3, 'model__min_samples_leaf': 1, 'model__min_samples_split': 5}\n",
            "Лучший MAE на CV: 0.38527464244999027\n",
            "\n",
            "MyDecisionTreeRegressor + логарифм таргета\n",
            "MAE: 9.653658712273353\n",
            "RMSE: 11.483049920745986\n",
            "R^2: 0.2709606451414325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В регрессионной задаче по ценам недвижимости дерево решений в «сыром» виде даёт провальные результаты, а основное улучшение приходит от двух шагов тюнинга структуры (max_depth, min_samples_split, min_samples_leaf) и перехода к логарифму таргета. Лог(distance) в признаках практически ничего не меняет. Собственная реализация MyDecisionTreeRegressor по всем сценариям воспроизводит поведение sklearn-дерева: метрики почти совпадают, те же гиперпараметры оказываются оптимальными, а лучший результат даёт именно логарифмирование цены. Это позволяет считать реализацию корректной, а главный вывод — как и в прошлых лабораторных, качество в итоге определяется не столько ручным кодом модели, сколько грамотной трансформацией целевой переменной и регуляризацией структуры дерева"
      ],
      "metadata": {
        "id": "_VFXaxGTJSId"
      }
    }
  ]
}