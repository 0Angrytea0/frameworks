{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HfZYjAgS4DKB"
      },
      "outputs": [],
      "source": [
        "#KGAT_746a8c48819a19cbaf8ca0048244831b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAteFYHa0k1i",
        "outputId": "9565f790-da6e-42eb-cf4e-ec3bcf6636be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl88susl6beB"
      },
      "source": [
        "Скачиваем датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLAcqRm923If",
        "outputId": "10713beb-0be9-49f3-d3e0-3be2982679c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: angrytea\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/syahrulapriansyah2/myanimelist-2025\n",
            "Downloading myanimelist-2025.zip to ./myanimelist-2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.57M/9.57M [00:00<00:00, 352MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/syahrulapriansyah2/myanimelist-2025\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64lS8fF6fBt"
      },
      "source": [
        "Библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aNDSDuIb371_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvYGk-dl6ogT"
      },
      "source": [
        "Чтение датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Pl-1TcL3GIl"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_columns\", None)\n",
        "df = pd.read_csv(\"myanimelist-2025/mal_anime.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFH5vx7q6t1S"
      },
      "source": [
        "Удаляем строки, не имеющие в Members значения  \n",
        "Задаем условие успешности: тайтл успешный, если у него 75% и больше зрителей: 1 - аниме успешное (собрало много просмотров) 0 - не успешное  \n",
        "Для признаков возьмем Source, Type, Released_Season, Episodes, Genres, Studios, Demographic  \n",
        "Пропуски по эпизодам заменяем медианой, по категориальным признакам на Unknown, жанрам и студиям поставим пустые строки  \n",
        "Создаем список жанров  \n",
        "Ищем самые популярные жанры и делаем для них колонки  \n",
        "Список студий делаем  \n",
        "Ищем популярные студии, не учитывая 'add some'  \n",
        "Задаем колонки  \n",
        "Собираем матрицу X из фич и назначаем целевую переменную успешности y  \n",
        "Делим данные на train и test  \n",
        "Числовые и бинарные признаки будем масштабировать, а для категориальных используем one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ymYEQo-b3Jkr"
      },
      "outputs": [],
      "source": [
        "df[\"Members\"] = pd.to_numeric(df[\"Members\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"Members\"])\n",
        "th = df[\"Members\"].quantile(0.75)\n",
        "\n",
        "df[\"hit\"] = (df[\"Members\"] >= th).astype(int)\n",
        "df[\"Episodes\"] = pd.to_numeric(df[\"Episodes\"], errors=\"coerce\")\n",
        "m_episodes = df[\"Episodes\"].median()\n",
        "df[\"Episodes\"] = df[\"Episodes\"].fillna(m_episodes)\n",
        "\n",
        "for col in [\"Source\", \"Type\", \"Released_Season\", \"Demographic\"]:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "for col in [\"Genres\", \"Studios\"]: df[col] = df[col].fillna(\"\")\n",
        "\n",
        "df[\"Genres_l\"] = df[\"Genres\"].apply(lambda x: [i.strip() for i in str(x).split(\",\") if i.strip() != \"\"])\n",
        "genre_count = Counter()\n",
        "\n",
        "for g in df[\"Genres_l\"]:\n",
        "    genre_count.update(g)\n",
        "\n",
        "top_g = [i for i, _ in genre_count.most_common(10)]\n",
        "\n",
        "for g in top_g:\n",
        "    col_name = f\"genre_{g}\"\n",
        "    df[col_name] = df[\"Genres_l\"].apply(lambda lst: int(g in lst))\n",
        "\n",
        "g_cols = [f\"genre_{g}\" for g in top_g]\n",
        "df[\"Studios_l\"] = df[\"Studios\"].apply(lambda x: [i.strip() for i in str(x).split(\",\") if i.strip() != \"\"])\n",
        "\n",
        "studio_count = Counter()\n",
        "for s in df[\"Studios_l\"]:\n",
        "  for st in s:\n",
        "    clean_s = st.strip()\n",
        "    if \"add some\" in clean_s.lower(): continue\n",
        "    studio_count[clean_s] += 1\n",
        "\n",
        "top_s = [i for i, _ in studio_count.most_common(10)]\n",
        "for s in top_s:\n",
        "    col_name = f\"studio_{s}\"\n",
        "    df[col_name] = df[\"Studios_l\"].apply(lambda lst: int(s in lst))\n",
        "\n",
        "studio_cols = [f\"studio_{s}\" for s in top_s]\n",
        "\n",
        "num = [\"Episodes\"] + g_cols + studio_cols\n",
        "categ = [\"Source\", \"Type\", \"Released_Season\", \"Demographic\"]\n",
        "feature = num + categ\n",
        "\n",
        "X = df[feature]\n",
        "y = df[\"hit\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
        "pred = ColumnTransformer(transformers=[\n",
        "    (\"num\", StandardScaler(),num),\n",
        "    (\"categ\", OneHotEncoder(handle_unknown=\"ignore\"), categ)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFx1LWE69Do"
      },
      "source": [
        "Строю бейзлайн-модель решающего дерева для задачи классификации. Использую DecisionTreeClassifier со стандартными параметрами, обучаю на подготовленных признаках и считаю метрики качества на тесте: Accuracy, F1 и ROC-AUC, а также подробный отчёт по классам"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайновое решающее дерево даёт Accuracy ≈ 0.74 и F1 ≈ 0.37 по классу hit, при этом ROC-AUC около 0.60. Модель хорошо распознаёт неуспешные тайтлы (класс 0: precision 0.79, recall 0.88), но по успешным тайтлам (класс 1) сильно просаживается recall (0.31) и, как следствие, F1. То есть базовое дерево явно склонно к консервативным предсказаниям класса 0 и не очень уверенно выделяет действительно популярные аниме"
      ],
      "metadata": {
        "id": "Y0l98SBMAOgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccN9S_iW3_8n",
        "outputId": "124cdd43-70d9-432f-a4bc-e2a3064154e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "Accuracy: 0.7352697095435685\n",
            "F1-score: 0.3708086785009862\n",
            "ROC-AUC: 0.6048510117122468\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       903\n",
            "           1       0.46      0.31      0.37       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.59      0.60      1205\n",
            "weighted avg       0.71      0.74      0.72      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tree_base = Pipeline(steps=[\n",
        "    (\"pred\", pred),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "tree_base.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = tree_base.predict(X_test)\n",
        "y_proba_tree = tree_base.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_tree))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_tree))\n",
        "print(classification_report(y_test, y_pred_tree))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kb1OiFC7IxY"
      },
      "source": [
        "Первая гипотеза - улучшить качество дерева за счёт подбора гиперпараметров структуры. Чищу переобучение через max_depth, min_samples_split, min_samples_leaf и критерий разбиения (gini/entropy). Использую GridSearchCV с оптимизацией F1-меры по положительному классу hit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подбор max_depth, min_samples_split и min_samples_leaf через GridSearchCV фактически вернул нам исходные настройки дерева (параметры совпали с дефолтом), и итоговые метрики полностью совпали с бейзлайном: Accuracy ≈ 0.74, F1 ≈ 0.37, ROC-AUC ≈ 0.60. Это говорит о том, что на текущем наборе признаков и при выбранной метрике (F1 по hit) автоматический подбор структуры дерева не даёт дополнительного выигрыша по качеству по сравнению с базовой конфигурацией"
      ],
      "metadata": {
        "id": "6zmR69OiAaXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EcUHMl66ZWV",
        "outputId": "ac81c5ed-90f2-4d50-a89d-dd480aad191a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
            "Лучший F1 на CV: 0.36353307583885536\n",
            "\n",
            "Гипотеза 1\n",
            "Accuracy: 0.7352697095435685\n",
            "F1-score: 0.3708086785009862\n",
            "ROC-AUC: 0.6048510117122468\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       903\n",
            "           1       0.46      0.31      0.37       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.59      0.60      1205\n",
            "weighted avg       0.71      0.74      0.72      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "param_grid_tree1 = {\n",
        "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
        "    \"clf__max_depth\": [None, 5, 10, 15, 20],\n",
        "    \"clf__min_samples_split\": [2, 10, 50],\n",
        "    \"clf__min_samples_leaf\": [1, 5, 20]\n",
        "}\n",
        "\n",
        "tree_clf1 = Pipeline(steps=[\n",
        "    (\"pred\", pred),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "grid_tree1 = GridSearchCV(\n",
        "    estimator=tree_clf1,\n",
        "    param_grid=param_grid_tree1,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_tree1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_tree1.best_params_)\n",
        "print(\"Лучший F1 на CV:\", grid_tree1.best_score_)\n",
        "\n",
        "best_tree1 = grid_tree1.best_estimator_\n",
        "y_pred_tree1 = best_tree1.predict(X_test)\n",
        "y_proba_tree1 = best_tree1.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree1))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_tree1))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_tree1))\n",
        "print(classification_report(y_test, y_pred_tree1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiRCQiPB7RG6"
      },
      "source": [
        "Вторая гипотеза - заменить исходное количество эпизодов на логарифм log(1 + Episodes), чтобы сгладить распределение этого признака. Строю новый набор признаков с log_Episodes, заново делю на train/test, настраиваю ColumnTransformer и снова подбираю гиперпараметры дерева по F1-мере"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавление признака log_Episodes и повторный подбор гиперпараметров тоже не изменили итоговых метрик: Accuracy, F1 и ROC-AUC остались на уровне бейзлайна. То есть решающее дерево либо уже «видит» нужную информацию в сырых Episodes, либо влияние этого признака в целом невелико по сравнению с жанрами, студиями и категориальными фичами. С точки зрения качества модель ведёт себя идентично гипотезе 1, так что логарифмирование числа эпизодов в контексте дерева решений заметной пользы не даёт"
      ],
      "metadata": {
        "id": "c4eYwCADAfDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0_5gQCJ7XfG",
        "outputId": "c639a333-b552-4b95-d620-39d1776dd28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
            "Лучший F1 на CV: 0.3653184854459217\n",
            "\n",
            "Decision Tree + log(Episodes)\n",
            "Accuracy: 0.7352697095435685\n",
            "F1-score: 0.3708086785009862\n",
            "ROC-AUC: 0.6048510117122468\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       903\n",
            "           1       0.46      0.31      0.37       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.59      0.60      1205\n",
            "weighted avg       0.71      0.74      0.72      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = df.copy()\n",
        "df2[\"log_Episodes\"] = np.log1p(df2[\"Episodes\"])\n",
        "\n",
        "num2 = [\"log_Episodes\"] + g_cols + studio_cols\n",
        "categ = [\"Source\", \"Type\", \"Released_Season\", \"Demographic\"]\n",
        "feature2 = num2 + categ\n",
        "\n",
        "X2 = df2[feature2]\n",
        "y2 = df2[\"hit\"]\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X2,\n",
        "    y2,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y2\n",
        ")\n",
        "\n",
        "pred2 = ColumnTransformer(transformers=[\n",
        "    (\"num2\", StandardScaler(), num2),\n",
        "    (\"categ\", OneHotEncoder(handle_unknown=\"ignore\"), categ),\n",
        "])\n",
        "\n",
        "tree_clf2 = Pipeline(steps=[\n",
        "    (\"pred\", pred2),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_tree2 = param_grid_tree1\n",
        "\n",
        "grid_tree2 = GridSearchCV(\n",
        "    estimator=tree_clf2,\n",
        "    param_grid=param_grid_tree2,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_tree2.fit(X2_train, y2_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_tree2.best_params_)\n",
        "print(\"Лучший F1 на CV:\", grid_tree2.best_score_)\n",
        "\n",
        "best_tree2 = grid_tree2.best_estimator_\n",
        "y_pred_tree2 = best_tree2.predict(X2_test)\n",
        "y_proba_tree2 = best_tree2.predict_proba(X2_test)[:, 1]\n",
        "\n",
        "print(\"\\nDecision Tree + log(Episodes)\")\n",
        "print(\"Accuracy:\", accuracy_score(y2_test, y_pred_tree2))\n",
        "print(\"F1-score:\", f1_score(y2_test, y_pred_tree2))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y2_test, y_proba_tree2))\n",
        "print(classification_report(y2_test, y_pred_tree2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdBNTWVM77AZ"
      },
      "source": [
        "Третья гипотеза - увеличить размер one-hot-представления жанров и студий до топ-20 по частоте, чтобы дать дереву больше информативных бинарных признаков. Заново формирую признаки Episodes + top-20 жанров/студий, делю выборку, настраиваю препроцессинг и подбираю гиперпараметры дерева по F1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При использовании топ-20 жанров и студий вместо топ-10 F1 по классу hit даже немного просел (с ≈0.37 до ≈0.34), Accuracy тоже слегка упала (до ≈0.73), хотя ROC-AUC чуть вырос (≈0.62). Это можно интерпретировать так: модель стала чуть лучше ранжировать объекты по вероятности быть hit в целом, но на фиксованном пороге 0.5 стала ошибаться чаще в терминах F1 (особенно по редкому классу). Дополнительные бинарные признаки для более редких жанров/студий, по сути, добавляют шум и легко переобучают дерево, поэтому расширение до top-20 в нашей постановке задачи не улучшило, а скорее ухудшило качество по основной целевой метрике"
      ],
      "metadata": {
        "id": "xuzoLI97Amqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QDjV3szu8B4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45aaa5c4-4391-416a-b6ea-fec60946b5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'clf__criterion': 'entropy', 'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}\n",
            "Лучший F1 на CV: 0.3878221423007429\n",
            "\n",
            "Decision Tree + top-20 жанров/студий\n",
            "Accuracy: 0.7311203319502074\n",
            "F1-score: 0.3441295546558704\n",
            "ROC-AUC: 0.617417658577369\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       903\n",
            "           1       0.44      0.28      0.34       302\n",
            "\n",
            "    accuracy                           0.73      1205\n",
            "   macro avg       0.61      0.58      0.59      1205\n",
            "weighted avg       0.70      0.73      0.71      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "top_s20 = [i for i, _ in studio_count.most_common(20)]\n",
        "for s in top_s20:\n",
        "    col_name = f\"studio_{s}\"\n",
        "    df[col_name] = df[\"Studios_l\"].apply(lambda lst: int(s in lst))\n",
        "studio_cols20 = [f\"studio_{s}\" for s in top_s20]\n",
        "\n",
        "top_g20 = [i for i, _ in genre_count.most_common(20)]\n",
        "for g in top_g20:\n",
        "    col_name = f\"genre_{g}\"\n",
        "    df[col_name] = df[\"Genres_l\"].apply(lambda lst: int(g in lst))\n",
        "g_cols20 = [f\"genre_{g}\" for g in top_g20]\n",
        "\n",
        "num3 = [\"Episodes\"] + g_cols20 + studio_cols20\n",
        "categ3 = [\"Source\", \"Type\", \"Released_Season\", \"Demographic\"]\n",
        "feature3 = num3 + categ3\n",
        "\n",
        "X3 = df[feature3]\n",
        "y3 = df[\"hit\"]\n",
        "\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(\n",
        "    X3,\n",
        "    y3,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y3\n",
        ")\n",
        "\n",
        "pred3 = ColumnTransformer(transformers=[\n",
        "    (\"num\", StandardScaler(), num3),\n",
        "    (\"categ\", OneHotEncoder(handle_unknown=\"ignore\"), categ3),\n",
        "])\n",
        "\n",
        "tree_clf3 = Pipeline(steps=[\n",
        "    (\"pred\", pred3),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_tree3 = param_grid_tree1\n",
        "\n",
        "grid_tree3 = GridSearchCV(\n",
        "    estimator=tree_clf3,\n",
        "    param_grid=param_grid_tree3,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_tree3.fit(X3_train, y3_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_tree3.best_params_)\n",
        "print(\"Лучший F1 на CV:\", grid_tree3.best_score_)\n",
        "\n",
        "best_tree3 = grid_tree3.best_estimator_\n",
        "y_pred_tree3 = best_tree3.predict(X3_test)\n",
        "y_proba_tree3 = best_tree3.predict_proba(X3_test)[:, 1]\n",
        "\n",
        "print(\"\\nDecision Tree + top-20 жанров/студий\")\n",
        "print(\"Accuracy:\", accuracy_score(y3_test, y_pred_tree3))\n",
        "print(\"F1-score:\", f1_score(y3_test, y_pred_tree3))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y3_test, y_proba_tree3))\n",
        "print(classification_report(y3_test, y_pred_tree3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoL7pdDc8ZJY"
      },
      "source": [
        "Реализую собственный классификатор дерева решений  \n",
        "Модель поддерживает бинарную классификацию и критерий качества Gini. При обучении в методе fit я сохраняю исходные классы, а затем рекурсивно строю дерево в виде вложенных словарей. На каждом узле считаю Gini-нечистоту, перебираю все признаки и возможные пороги (уникальные значения признака), считаю взвешенную Gini для левой и правой части и выбираю разбиение с минимальной нечистотой с учётом ограничений max_depth, min_samples_split и min_samples_leaf. Если дальше делить узел не имеет смысла (мало объектов, достигнута максимальная глубина или все объекты одного класса), создаётся лист, в котором хранится количество объектов каждого класса. В predict_proba я для каждого объекта спускаюсь по дереву (сравнивая значение признака с порогом) до листа и возвращаю нормализованные частоты классов как вероятности, а в predict беру класс с максимальной вероятностью и мапплю его обратно в исходные меткии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iR-712xN8ZcX"
      },
      "outputs": [],
      "source": [
        "class MyDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=None\n",
        "    ):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_classes_ = self.classes_.shape[0]\n",
        "        if self.n_classes_ != 2:\n",
        "            raise ValueError(\"MyDecisionTreeClassifier сейчас поддерживает только бинарную классификацию\")\n",
        "\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "\n",
        "        self.n_features_ = X.shape[1]\n",
        "        self.tree_ = self._build_tree(X, y, depth=0, rng=rng)\n",
        "        return self\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = y.shape[0]\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        p = counts / m\n",
        "        return 1.0 - np.sum(p ** 2)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        m, n = X.shape\n",
        "        if m < self.min_samples_split:\n",
        "            return None, None\n",
        "\n",
        "        best_gini = None\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        current_gini = self._gini(y)\n",
        "\n",
        "        for feature_idx in range(n):\n",
        "            x_col = X[:, feature_idx]\n",
        "            thresholds = np.unique(x_col)\n",
        "            if thresholds.shape[0] == 1:\n",
        "                continue\n",
        "\n",
        "            for t in thresholds:\n",
        "                left_mask = x_col <= t\n",
        "                right_mask = x_col > t\n",
        "\n",
        "                n_left = np.sum(left_mask)\n",
        "                n_right = np.sum(right_mask)\n",
        "\n",
        "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                g_left = self._gini(y[left_mask])\n",
        "                g_right = self._gini(y[right_mask])\n",
        "\n",
        "                g_split = (n_left * g_left + n_right * g_right) / m\n",
        "\n",
        "                if best_gini is None or g_split < best_gini:\n",
        "                    best_gini = g_split\n",
        "                    best_feature = feature_idx\n",
        "                    best_threshold = t\n",
        "\n",
        "        if best_gini is None or best_gini >= current_gini:\n",
        "            return None, None\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _build_tree(self, X, y, depth, rng):\n",
        "        node = {}\n",
        "        num_samples = y.shape[0]\n",
        "        num_labels = np.unique(y).shape[0]\n",
        "\n",
        "        if num_labels == 1:\n",
        "            node[\"type\"] = \"leaf\"\n",
        "            node[\"class_counts\"] = self._class_counts(y)\n",
        "            return node\n",
        "\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            node[\"type\"] = \"leaf\"\n",
        "            node[\"class_counts\"] = self._class_counts(y)\n",
        "            return node\n",
        "\n",
        "        if num_samples < self.min_samples_split:\n",
        "            node[\"type\"] = \"leaf\"\n",
        "            node[\"class_counts\"] = self._class_counts(y)\n",
        "            return node\n",
        "\n",
        "        feature_idx, threshold = self._best_split(X, y)\n",
        "\n",
        "        if feature_idx is None:\n",
        "            node[\"type\"] = \"leaf\"\n",
        "            node[\"class_counts\"] = self._class_counts(y)\n",
        "            return node\n",
        "\n",
        "        left_mask = X[:, feature_idx] <= threshold\n",
        "        right_mask = X[:, feature_idx] > threshold\n",
        "\n",
        "        X_left, y_left = X[left_mask], y[left_mask]\n",
        "        X_right, y_right = X[right_mask], y[right_mask]\n",
        "\n",
        "        node[\"type\"] = \"internal\"\n",
        "        node[\"feature_idx\"] = feature_idx\n",
        "        node[\"threshold\"] = threshold\n",
        "        node[\"left\"] = self._build_tree(X_left, y_left, depth + 1, rng)\n",
        "        node[\"right\"] = self._build_tree(X_right, y_right, depth + 1, rng)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def _class_counts(self, y):\n",
        "        counts = np.zeros(self.n_classes_, dtype=float)\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            counts[idx] = np.sum(y == c)\n",
        "        return counts\n",
        "\n",
        "    def _predict_one_proba(self, x, node):\n",
        "        if node[\"type\"] == \"leaf\":\n",
        "            counts = node[\"class_counts\"]\n",
        "            total = np.sum(counts)\n",
        "            if total == 0:\n",
        "                return np.ones(self.n_classes_) / self.n_classes_\n",
        "            return counts / total\n",
        "\n",
        "        if x[node[\"feature_idx\"]] <= node[\"threshold\"]:\n",
        "            return self._predict_one_proba(x, node[\"left\"])\n",
        "        else:\n",
        "            return self._predict_one_proba(x, node[\"right\"])\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        proba = np.zeros((n_samples, self.n_classes_), dtype=float)\n",
        "        for i in range(n_samples):\n",
        "            proba[i] = self._predict_one_proba(X[i], self.tree_)\n",
        "        return proba\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        idx = np.argmax(proba, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVKTSRv78Z5H"
      },
      "source": [
        "Строю бейзлайн на собственной реализации дерева решений  \n",
        "Использую те же признаки и препроцессинг, что и для sklearn-дерева: StandardScaler для числовых признаков, one-hot для категориальных. Параметры дерева оставляю “по умолчанию” (неограниченная глубина, min_samples_split=2, min_samples_leaf=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайновое дерево в моей реализации даёт Accuracy ≈ 0.74, F1 ≈ 0.37 и ROC-AUC ≈ 0.61. Картинка очень похожа на sklearn-дерево: класс 0 определяется хорошо (precision 0.79, recall 0.88), а успешные тайтлы (класс 1) по-прежнему ловим неидеально (recall 0.31, F1 около 0.37). Это значит, что базовое поведение моего дерева практически совпадает с библиотечной реализацией: модель уверенно узнаёт неуспешные аниме и осторожно прогнозирует класс hit"
      ],
      "metadata": {
        "id": "oAD66wuECNKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SijCEVZo8aJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f5d10d-3cf6-416c-c3df-dd1a52453275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бейзлайн\n",
            "Accuracy: 0.7394190871369295\n",
            "F1-score: 0.3745019920318725\n",
            "ROC-AUC: 0.6113910218330363\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.84       903\n",
            "           1       0.47      0.31      0.37       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.60      0.60      1205\n",
            "weighted avg       0.71      0.74      0.72      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "my_tree_base = Pipeline(steps=[\n",
        "    (\"pred\", pred),\n",
        "    (\"clf\", MyDecisionTreeClassifier(\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "my_tree_base.fit(X_train, y_train)\n",
        "\n",
        "y_pred_my_tree = my_tree_base.predict(X_test)\n",
        "y_proba_my_tree = my_tree_base.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Бейзлайн\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_my_tree))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_my_tree))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_my_tree))\n",
        "print(classification_report(y_test, y_pred_my_tree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnEsOXjx9VIy"
      },
      "source": [
        "Гипотеза 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV для MyDecisionTreeClassifier выбрал параметры, фактически совпадающие с бейзлайном, поэтому итоговые метрики полностью совпали с базовой моделью: Accuracy ≈ 0.74, F1 ≈ 0.37, ROC-AUC ≈ 0.61. Это означает, что внутри моей реализации та же логика роста дерева, и дополнительный перебор глубины/минимального размера узлов в заданной сетке не даёт прироста качества. По сути, гипотеза 1 лишь подтвердило, что настройка структуры дерева в таком виде не улучшает F1 по классу hit"
      ],
      "metadata": {
        "id": "wAGt4w95CRgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol3ZuHNX9Yhl",
        "outputId": "2836c86d-39a6-48b2-a60d-7d07cfa171df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
            "Лучший F1 на CV: 0.36242371191736306\n",
            "\n",
            "Гипотеза 1\n",
            "Accuracy: 0.7394190871369295\n",
            "F1-score: 0.3745019920318725\n",
            "ROC-AUC: 0.6113910218330363\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.84       903\n",
            "           1       0.47      0.31      0.37       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.60      0.60      1205\n",
            "weighted avg       0.71      0.74      0.72      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "param_grid_my_tree1 = {\n",
        "    \"clf__max_depth\": [None, 5, 10, 15, 20],\n",
        "    \"clf__min_samples_split\": [2, 10, 50],\n",
        "    \"clf__min_samples_leaf\": [1, 5, 20]\n",
        "}\n",
        "\n",
        "my_tree_clf1 = Pipeline(steps=[\n",
        "    (\"pred\", pred),\n",
        "    (\"clf\", MyDecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "grid_my_tree1 = GridSearchCV(\n",
        "    estimator=my_tree_clf1,\n",
        "    param_grid=param_grid_my_tree1,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_my_tree1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_my_tree1.best_params_)\n",
        "print(\"Лучший F1 на CV:\", grid_my_tree1.best_score_)\n",
        "\n",
        "best_my_tree1 = grid_my_tree1.best_estimator_\n",
        "\n",
        "y_pred_my1 = best_my_tree1.predict(X_test)\n",
        "y_proba_my1 = best_my_tree1.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nГипотеза 1\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_my1))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_my1))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_my1))\n",
        "print(classification_report(y_test, y_pred_my1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6l3YlLo9aaa"
      },
      "source": [
        "Гипотеза 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Во второй гипотезе я логарифмировала число эпизодов и снова провела подбор гиперпараметров для MyDecisionTreeClassifier. Результаты снова ровно совпали с бейзлайном: те же лучшие параметры и те же значения Accuracy, F1 и ROC-AUC. Для дерева решений это ожидаемо: оно умеет само находить пороги по исходным значениям Episodes, и монотонное преобразование вида log не даёт ему дополнительной силы. В итоге логарифмирование эпизодов никак не меняет качество классификации в рамках используемой модели"
      ],
      "metadata": {
        "id": "r8_3yAYaCWa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yLQRpzg9b7k",
        "outputId": "eeba75b2-41c7-43fa-b720-c24279493dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
            "Лучший F1 на CV: 0.36242371191736306\n",
            "\n",
            "MyDecisionTreeClassifier + log(Episodes)\n",
            "Accuracy: 0.7394190871369295\n",
            "F1-score: 0.3745019920318725\n",
            "ROC-AUC: 0.6113910218330363\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.84       903\n",
            "           1       0.47      0.31      0.37       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.60      0.60      1205\n",
            "weighted avg       0.71      0.74      0.72      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "my_tree_clf2 = Pipeline(steps=[\n",
        "    (\"pred\", pred2),\n",
        "    (\"clf\", MyDecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_my_tree2 = param_grid_my_tree1\n",
        "\n",
        "grid_my_tree2 = GridSearchCV(\n",
        "    estimator=my_tree_clf2,\n",
        "    param_grid=param_grid_my_tree2,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_my_tree2.fit(X2_train, y2_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_my_tree2.best_params_)\n",
        "print(\"Лучший F1 на CV:\", grid_my_tree2.best_score_)\n",
        "\n",
        "best_my_tree2 = grid_my_tree2.best_estimator_\n",
        "\n",
        "y_pred_my2 = best_my_tree2.predict(X2_test)\n",
        "y_proba_my2 = best_my_tree2.predict_proba(X2_test)[:, 1]\n",
        "\n",
        "print(\"\\nMyDecisionTreeClassifier + log(Episodes)\")\n",
        "print(\"Accuracy:\", accuracy_score(y2_test, y_pred_my2))\n",
        "print(\"F1-score:\", f1_score(y2_test, y_pred_my2))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y2_test, y_proba_my2))\n",
        "print(classification_report(y2_test, y_pred_my2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC6b0i4M9gNt"
      },
      "source": [
        "Гипотеза 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При добавлении top-20 жанров и студий Accuracy чуть выросла (≈0.742), ROC-AUC тоже подрос до ≈0.64, но F1 по классу hit заметно просел — примерно до 0.34. То есть дерево стало ещё сильнее фокусироваться на корректном предсказании основного класса 0 (неуспешные тайтлы, recall 0.90), но по редкому классу 1 начало ошибаться чаще (recall 0.26 и более низкий F1). Такое поведение похоже на лёгкое переобучение на дополнительные редкие бинарные признаки, которые добавляют шум, а не полезную структуру для нашей ключевой метрики"
      ],
      "metadata": {
        "id": "TNc14IiYCgR-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWQ0pSzD9haj",
        "outputId": "3d7c2352-1f41-4048-eae1-952643053ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}\n",
            "Лучший F1 на CV: 0.37975976662067296\n",
            "\n",
            "MyDecisionTreeClassifier + top-20 жанров/студий\n",
            "Accuracy: 0.7419087136929461\n",
            "F1-score: 0.33970276008492567\n",
            "ROC-AUC: 0.6441002398187059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84       903\n",
            "           1       0.47      0.26      0.34       302\n",
            "\n",
            "    accuracy                           0.74      1205\n",
            "   macro avg       0.63      0.58      0.59      1205\n",
            "weighted avg       0.71      0.74      0.71      1205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "my_tree_clf3 = Pipeline(steps=[\n",
        "    (\"pred\", pred3),\n",
        "    (\"clf\", MyDecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_my_tree3 = param_grid_my_tree1\n",
        "\n",
        "grid_my_tree3 = GridSearchCV(\n",
        "    estimator=my_tree_clf3,\n",
        "    param_grid=param_grid_my_tree3,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_my_tree3.fit(X3_train, y3_train)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_my_tree3.best_params_)\n",
        "print(\"Лучший F1 на CV:\", grid_my_tree3.best_score_)\n",
        "\n",
        "best_my_tree3 = grid_my_tree3.best_estimator_\n",
        "\n",
        "y_pred_my3 = best_my_tree3.predict(X3_test)\n",
        "y_proba_my3 = best_my_tree3.predict_proba(X3_test)[:, 1]\n",
        "\n",
        "print(\"\\nMyDecisionTreeClassifier + top-20 жанров/студий\")\n",
        "print(\"Accuracy:\", accuracy_score(y3_test, y_pred_my3))\n",
        "print(\"F1-score:\", f1_score(y3_test, y_pred_my3))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y3_test, y_proba_my3))\n",
        "print(classification_report(y3_test, y_pred_my3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В задаче предсказания успешности аниме мои эксперименты с решающим деревом показали, что и библиотечная, и собственная реализация дают очень похожие результаты: Accuracy около 0.73–0.74, F1 по классу hit на уровне ≈0.37 и ROC-AUC чуть выше 0.6. Тюнинг гиперпараметров дерева (глубина, минимальный размер узла и т.д.) и простые преобразования признаков (log(Episodes), расширение списка жанров и студий до top-20) либо практически не меняют качество, либо даже немного ухудшают F1 по целевому классу. При этом моя реализация дерева решений воспроизводит поведение sklearn-модели как по настройкам, которые выбирает GridSearchCV, так и по итоговым метрикам, поэтому её можно считать корректной. В целом решающее дерево на этом датасете даёт умеренное качество и сильно уступает по F1 более «гладким» моделям вроде логистической регрессии, а ключевую роль снова играет не столько сам алгоритм, сколько разумный выбор признаков и целевой метрики"
      ],
      "metadata": {
        "id": "r99K8c6rCqwj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}